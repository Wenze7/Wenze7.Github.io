{"pages":[{"title":"About","text":"QQ:1245666720WeChat:peng1245666720SDKD_SE_17_1","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Apple设备之间接力无法使用","text":"最近我的iphone和MAC之间不能使用接力了，费了老大劲才找到解决办法 确保按照官方指示开启所要求的功能 所有设备均使用同一 Apple ID 登录 iCloud。 所有设备均已开启蓝牙。 所有 Mac、iPhone、iPad 或 iPod touch 均已开启 Wi-Fi。 所有设备均已开启接力： 在 Mac 上，选取左上角的苹果菜单 -“系统偏好设置”-“通用”。选中“允许在这台 Mac 和 iCloud 设备之间使用接力”。 在 iPhone、iPad 或 iPod touch 上，前往“设置”-“通用”-“接力”，然后开启接力。 （支持接力的应用包括“邮件”、“地图”、“Safari 浏览器”、“提醒事项”、“日历”、“通讯录”、“Pages 文稿”、“Numbers 表格”、“Keynote 讲演”，以及众多第三方应用。） 上述操作进行后仍不行，请关闭SIP(系统完整性保护) 终端输入csrutil status查看SIP状态，如果是enable则是开启，是disable是关闭。 重启MAC，立即在键盘上按住 Command ⌘ + R，直到看到 Apple 标志，进入Recovery模式。 在上方的菜单栏点击「实用工具」选择「终端」。 在终端中，输入csrutil enable后回车。 点击菜单栏  标志，选择「重新启动」。这样接力就可以使用了。 如果要开启SIP，则重新进入Recovery模式，打开终端，输入csrutil enable即可。","link":"/2020/02/28/Apple%E8%AE%BE%E5%A4%87%E4%B9%8B%E9%97%B4%E6%8E%A5%E5%8A%9B%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8/"},{"title":"Cart树的原理及实现","text":"CART在回归预测上的原理及实现1.CART简介CART是指分类回归树，Classfication And Regression Tree，缩写为CART,CART算法采用二分递归分割的技术将当前样本集分为两个子样本集，使得生成的每个非叶子节点都有两个分支。所以CART的结构是二叉树，CART可以处理连续型变量和离散型变量，利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。 如果待预测分类是离散型数据，则CART生成分类决策树。 如果待预测分类是连续性数据，则CART生成回归决策树。 2.CART做回归预测在处理连续值时，CART采取回归的方式进行预测，假设X与Y分别是输入变量和输出变量，并且Y的取值是连续的，设训练集为D。$$D={(x_1,y_1),(x_2,y_2),(x_3,Y_3),…,(x_n,y_n)}$$CART的度量目标是，选取一个特征A和其对应的划分点s来划分数据集，使得划分出来的两个数据集的均方误差最小,不断的进行上述操作，当划分之后的均方差和划分之前的均方差之差的绝对值小于一个特定的阀值时，停止划分。$$min[min\\sum_{x_i\\in D_1(A,s)}(y_i-c_1)+min\\sum_{x_i\\in D_2(A,s)}(y_i-c_2)]$$其中$c_1$为$D_1$的样本输出均值，$c_2$为$D_2$的样本输出均值。在使用测试集时，采用均方误差作为预测误差。$$MSE=\\frac{1}{n}\\sum_{i=1}^n(predicted_i−label)^2$$其中$predicted_i$为预测值，$label$为实际预测值。 3.题目内容使用UCI公开数据集airfoil_self_noise（翼型自噪声），该数据集是NACA在2014年发布的一组关于0012翼型机在不同风洞速度和角度的数据，包含6个属性，分别是频率、角度、炫长、自由流速度、吸力侧位移厚度以及输出的y值低压声等级。该数据统计信息如下： 数据 统计值 example 1503 Training set 1200 Test set 303 Range of y 103.38-140.987 attributes 6 4.具体实现 结点的存储采用对象的方式进行，相比与字典的存储方式，虽代码量大但结构清晰。12345678910111213141516class Node: faNode = None #父节点 leftNode = None #左分支 rightNode = None #右分支 items = [] #划分到当前分支的数据集 attributes = [] #特征 label = 0 #划分这个节点的特征 num = 0 #划分的值或者输出的值 isLeaf = 0 #判断当前节点是否为叶节点 isLeft = 0 #判断当前节点是否为左分支，主要用于可视化 def __init__(self,items,attributes,label,num,isLeaf,): self.items = items self.attributes = attributes self.label = label self.num = num self.isLeaf = isLeaf 树的创建1234找到最佳的待切分特征：如果该节点不能再分，将该节点存为叶节点,执行二元切分在右子树调用 createTree() 方法在左子树调用 createTree() 方法 123456789101112131415161718192021222324252627282930313233343536373839404142def calCurVariance(tree,dataMat): ssum = 0;sum=0 for i in tree.items: ssum = ssum + dataMat[i][-1]*dataMat[i][-1] sum = sum + dataMat[i][-1] return sqrt(ssum-len(tree.items)*(sum/len(tree.items))*(sum/len(tree.items))),sum/len(tree.items)def createTree(tree,dataMat,Top): curVariance,y = calCurVariance(tree,dataMat) if curVariance &lt; 1e-5: tree.num = y tree.isLeaf=1 return best,tree.label,tree.num = findBestFeatureToSplit(tree,dataMat) if math.fabs(best-curVariance) &lt; Top: tree.isLeaf=1 tree.num = 0 for i in tree.items: tree.num = tree.num + dataMat[i][-1] tree.num = tree.num/len(tree.items) return tree.leftNode = Node([],[],'',0,0) tree.rightNode = Node([],[],'',0,0) tree.leftNode.faNode = tree tree.rightNode.faNode = tree for i in tree.items: if (dataMat[i][tree.label] &lt;= tree.num): tree.leftNode.items.append(i) else: tree.rightNode.items.append(i) for i in tree.attributes: tree.leftNode.attributes.append(i) tree.rightNode.attributes.append(i) tree.leftNode.isLeft=1 createTree(tree.leftNode,dataMat,Top) createTree(tree.rightNode,dataMat,Top) return calCurVariance() 用于计算当前节点内部的均方差。 createTree() 用于创建树节点，这个函数有3个参数 123tree : 当前节点dataMat : 数据集矩阵Top : 阀值 函数先计算当前节点内部的均方差，当均方差接近于0时，说明这个里面的数据集纯度已经很高，不再进行划分。 12345curVariance,y = calCurVariance(tree,dataMat)if curVariance &lt; 1e-5: tree.num = y # y是输出，即样本的输出均值 tree.isLeaf=1 return 然后找到最优的划分特征即特征点，判断是否划分 12345678best,tree.label,tree.num = findBestFeatureToSplit(tree,dataMat)if math.fabs(best-curVariance) &lt; Top: tree.isLeaf=1 tree.num = 0 for i in tree.items: tree.num = tree.num + dataMat[i][-1] tree.num = tree.num/len(tree.items) return 如果进行划分的话，就创建左右分支并且进行递归 12345678910111213141516tree.leftNode = Node([],[],'',0,0)tree.rightNode = Node([],[],'',0,0)tree.leftNode.faNode = treetree.rightNode.faNode = treefor i in tree.items:if (dataMat[i][tree.label] &lt;= tree.num): tree.leftNode.items.append(i)else: tree.rightNode.items.append(i)for i in tree.attributes:tree.leftNode.attributes.append(i)tree.rightNode.attributes.append(i)tree.leftNode.isLeft=1createTree(tree.leftNode,dataMat,Top)createTree(tree.rightNode,dataMat,Top) 树的划分 findBestFeatureToSplit() 用于寻找最优划分的特征和划分点，对于连续型的特征，可以把特征值从小到大排序，然后取两个相近特征值的均值作为候选划分点。假设特征值可以取$a_1,a_2,a_3,a_4,a_5,a_6$，那么可以取 $\\frac{a_1+a_2}{2},\\frac{a_2+a_3}{2},\\frac{a_3+a_4}{2},\\frac{a_4+a_5}{2},\\frac{a_5+a_6}{2}$ 这六个值当作候选的划分点，特征点小于或等于划分点的划分到左分支，大于特征点的划分到右分支。 12345678910111213141516def findBestFeatureToSplit(tree,dataMat): minVariance = sys.float_info.max attributeAns = 0 attributeNumAns = 0 for i in tree.attributes: curAttributeNum = [] for j in tree.items: curAttributeNum.append(dataMat[j][i]) j = j + 1 nowAns,attributeNum = calVariance(i,curAttributeNum,tree,dataMat) if(nowAns&lt;minVariance): minVariance = nowAns attributeAns = i attributeNumAns = attributeNum i = i + 1 return minVariance,attributeAns,attributeNumAns 对于每一个特征使用calVariance()函数找到其最佳的特征点，并与当前最优结果进行比较。 123456789101112131415161718192021222324252627282930313233343536373839404142434445def calVariance(curAttribute,curAttributeNum,tree,dataMat): attributeNum = 0 minVariance = sys.float_info.max lsum = 0;lssum = 0;ln = 0;rsum = 0;rssum = 0;rn = 0; class Pair: a = 0 b = 0 def __init__(self,a,b): self.a = a self.b = b def __lt__(self, other): return self.a &lt; other.a Pairs = [] for i in tree.items: Pairs.append(Pair(dataMat[i][curAttribute],dataMat[i][-1])) rn = rn + 1 rssum = rssum + dataMat[i][-1]*dataMat[i][-1] rsum = rsum + dataMat[i][-1] Pairs=sort(Pairs) curAttributeNum = sort(list(set(curAttributeNum))) i = 0 j = 0 while i &lt; len(curAttributeNum)-1: while Pairs[j].a &lt;= (curAttributeNum[i]+curAttributeNum[i+1])/2: ln = ln + 1 lsum = lsum + Pairs[j].b lssum = lssum + Pairs[j].b*Pairs[j].b rn = rn - 1 rsum = rsum - Pairs[j].b rssum = rssum - Pairs[j].b*Pairs[j].b j = j + 1 if math.fabs(lssum-ln*(lsum/ln)*(lsum/ln)) &lt; 0.001: leftAns = 0 else: leftAns = math.sqrt(lssum-ln*(lsum/ln)*(lsum/ln)) if math.fabs(rssum-rn*(rsum/rn)*(rsum/rn)) &lt; 0.001: rightAns = 0 else: rightAns = math.sqrt(rssum-rn*(rsum/rn)*(rsum/rn)) nowAns = leftAns + rightAns if(nowAns&lt;minVariance): minVariance = nowAns attributeNum = (curAttributeNum[i]+curAttributeNum[i+1])/2 i = i + 1 return minVariance,attributeNum 首先将每条数据的当前特质和当前特征值取出来，按照特征值的大小进行排序。 123456789101112131415class Pair: a = 0 b = 0 def __init__(self,a,b): self.a = a self.b = b def __lt__(self, other): return self.a &lt; other.aPairs = []for i in tree.items: Pairs.append(Pair(dataMat[i][curAttribute],dataMat[i][-1])) rn = rn + 1 rssum = rssum + dataMat[i][-1]*dataMat[i][-1] rsum = rsum + dataMat[i][-1]Pairs=sort(Pairs) 之后将数据集的特征值取出来，并按从小到大的顺序排好。 1curAttributeNum = sort(list(set(curAttributeNum))) 进行这两步操作是为了在遍历过程中，可以使复杂度达到线性，降低程序运行时间。之后进行具体候选特征值的均方差计算，求出最优结果。 123456789101112131415161718192021222324i = 0j = 0while i &lt; len(curAttributeNum)-1: while Pairs[j].a &lt;= (curAttributeNum[i]+curAttributeNum[i+1])/2: ln = ln + 1 lsum = lsum + Pairs[j].b lssum = lssum + Pairs[j].b*Pairs[j].b rn = rn - 1 rsum = rsum - Pairs[j].b rssum = rssum - Pairs[j].b*Pairs[j].b j = j + 1 if math.fabs(lssum-ln*(lsum/ln)*(lsum/ln)) &lt; 0.001: leftAns = 0 else: leftAns = math.sqrt(lssum-ln*(lsum/ln)*(lsum/ln)) if math.fabs(rssum-rn*(rsum/rn)*(rsum/rn)) &lt; 0.001: rightAns = 0 else: rightAns = math.sqrt(rssum-rn*(rsum/rn)*(rsum/rn)) nowAns = leftAns + rightAns if(nowAns&lt;minVariance): minVariance = nowAns attributeNum = (curAttributeNum[i]+curAttributeNum[i+1])/2 i = i + 1 5.剪枝剪枝的目的是为了防止过拟合，过拟合的意思就是当前模型对于训练集表现的很好，但是对验证数据集表现的很差，泛化能力很弱。这里的剪枝使用测试数据集进行剪枝，首先指定参数，使得构建出的树足够大、足够复杂，便于剪枝；接下来从上而下找到叶节点，用测试集来判断将这些叶节点合并是否能降低测试误差。如果是的话就合并。 12345基于已有的树切分测试数据： 如果存在任一子集是一棵树，则在该子集递归剪枝过程 计算将当前两个叶节点合并后的误差 计算不合并的误差 如果合并会降低误差的话，就将叶节点合并 calErrorNoMerge() 和 calErrorMerge() 用于计算两种处理方案的误差，以便于进行比较。 1234567891011121314151617def calErrorNoMerge(tree,lMat,rMat): error = 0 for item in lMat: val = item[-1] error = error + (val-tree.leftNode.num)*(val-tree.leftNode.num) for item in rMat: val = item[-1] error = error + (val-tree.rightNode.num)*(val-tree.rightNode.num) return errordef calErrorMerge(tree,testMat): error = 0 tree_num = (tree.leftNode.num+tree.rightNode.num)/2 for item in testMat: val = item[-1] error = error + (val-tree_num) * (val-tree_num) return error getMean() 是一个递归函数，它从上往下遍历树直到叶节点为止。如果找到两个叶节点则计算它们的平均值。该函数对树进行塌陷处理（即返回树平均值） 12345678910def getMean(tree): if tree.leftNode.isLeaf==0: getMean(tree.leftNode) if tree.rightNode.isLeaf==0: getMean(tree.rightNode) if tree.rightNode.isLeaf == 1 and tree.rightNode.isLeaf == 1: tree.num = (tree.rightNode.num+tree.leftNode.num)/2 tree.isLeaf = 1 tree.leftNode=tree.rightNode=None return splitSet() 用于划分测试集 1234567def splitSet(idx,num,testMat): lset = [] rset = [] for item in testMat: if(item[idx]&lt;=num): lset.append(item) else: rset.append(item) return lset,rset prune() 也是一个递归函数，接受的参数是当前节点和测试矩阵 12345678910111213141516171819def prune(tree,testMat): if(len(testMat)==0): getMean(tree) return lset,rset=splitSet(tree.label,tree.num,testMat) if tree.leftNode.isLeaf == 0: prune(tree.leftNode,lset) if tree.rightNode.isLeaf == 0: prune(tree.rightNode,rset) if tree.rightNode.isLeaf == 1 and tree.leftNode.isLeaf == 1: errorNoMerge = calErrorNoMerge(tree,lset,rset) errorMerge = calErrorMerge(tree,testMat) if(errorMerge&lt;errorNoMerge): tree.isLeaf = 1 tree.num = (tree.leftNode.num+tree.rightNode.num)/2 tree.leftNode = tree.rightNode = None print('merge') return return 首先判断测试集是否为空，如果为空则进行调用 getMean() 做塌陷处理。 123if(len(testMat)==0): getMean(tree) return 然后判断左分支和右分支是否存在，如果存在则进行递归。 1234if tree.leftNode.isLeaf == 0: prune(tree.leftNode,lset)if tree.rightNode.isLeaf == 0: prune(tree.rightNode,rset) 如果当前节点下有两个叶节点，则进行收否合并的判断。如果合并后的误差小于未合并的误差则进行合并，否则不进行合并。 123456789if tree.rightNode.isLeaf == 1 and tree.leftNode.isLeaf == 1: errorNoMerge = calErrorNoMerge(tree,lset,rset) errorMerge = calErrorMerge(tree,testMat) if(errorMerge&lt;errorNoMerge): tree.isLeaf = 1 tree.num = (tree.leftNode.num+tree.rightNode.num)/2 tree.leftNode = tree.rightNode = None print('merge') return 6.结果分析对于阀值的选择好坏是决定模型好坏的决定性因素，下表展示了在不同阀值下，模型对于同一测试数据集展现出的不同的误差。 阀值 剪枝前误差 剪枝后误差 7 31.454 29.234 6 14.911 10.351 5 13.283 8.722 4 13.665 7.467 3 11.887 7.327 2 11.417 6.789 1 8.379 5.699 0.5 4.485 2.259 0.1 4.565 2.342 从表中可以看出，当阀值在5及以下有着较小的误差，所以可以将阀值选择在5及以下。 7.可视化可视化用到是graphviz工具包 12345678910111213def printTree(tree,dot): if tree.isLeaf == 1: dot.node(name=str(tree),label='y=' + str(tree.num)) else: dot.node(name=str(tree), label=str(Label[tree.label]), color='green') if tree.faNode!=None: dot.edge(str(tree.faNode),str(tree),label='&lt;='+str(tree.faNode.num) if tree.isLeft == 1 else \"&gt;\"+str(tree.faNode.num)) if tree.isLeaf == 0: printTree(tree.leftNode,dot) printTree(tree.rightNode,dot)dot1 = Digraph(name=\"a\",comment=\"result1\",format=\"png\")printTree(root,dot1)dot1.view(filename=\"result\", directory=\"/Users/seven7777777/QQDownload/\") Digraph() 用于绘制一个图node() 用于绘制节点edge() 用于绘制边view() 用于保存图片并将图片显示出来 当取阀值为1时，生成的树： 点击此处查看 经过剪枝后：点击此处查看 ps:图片过大，加载需要一定时间 完整代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275from numpy import *import sysimport mathfrom graphviz import Digraphclass Node: faNode = None leftNode = None rightNode = None items = [] attributes = [] id = 0 label = 0 num = 0 isLeaf = 0 isLeft = 0 def __init__(self,items,attributes,label,num,isLeaf): self.items = items self.attributes = attributes self.label = label self.num = num self.isLeaf = isLeafdef loadFile(filengthame): dataMat = [] file = open(filengthame) for line in file.readlines(): curLine = line.strip().split('\\t') curLine = list(map(float,curLine)) dataMat.append(curLine) return dataMatdef loadFile1(filengthame): dataMat = [] file = open(filengthame) for line in file.readlines(): curLine = line.strip().split(' ') curLine = list(map(float,curLine)) dataMat.append(curLine) return dataMatdef init(): path = '/Users/seven7777777/QQDownload/airfoil_self_noise.dat' dataMat = loadFile(path) tree = Node([],[],'',0,0) i = 0 while i &lt; len(dataMat): tree.items.append(i) i = i + 1 i = 0 while i &lt; len(dataMat[0])-1: tree.attributes.append(i) i = i + 1 return tree,dataMatdef calVariance(curAttribute,curAttributeNum,tree,dataMat): attributeNum = 0 minVariance = sys.float_info.max lsum = 0;lssum = 0;ln = 0;rsum = 0;rssum = 0;rn = 0;dic = {} class Pair: a = 0 b = 0 def __init__(self,a,b): self.a = a self.b = b def __lt__(self, other): return self.a &lt; other.a Pairs = [] curAttributeNum = sort(list(set(curAttributeNum))) for i in tree.items: Pairs.append(Pair(dataMat[i][curAttribute],dataMat[i][-1])) rn = rn + 1 rssum = rssum + dataMat[i][-1]*dataMat[i][-1] rsum = rsum + dataMat[i][-1] Pairs=sort(Pairs) i = 0 j = 0 while i &lt; len(curAttributeNum)-1: while Pairs[j].a &lt;= (curAttributeNum[i]+curAttributeNum[i+1])/2: ln = ln + 1 lsum = lsum + Pairs[j].b lssum = lssum + Pairs[j].b*Pairs[j].b rn = rn - 1 rsum = rsum - Pairs[j].b rssum = rssum - Pairs[j].b*Pairs[j].b j = j + 1 if math.fabs(lssum-ln*(lsum/ln)*(lsum/ln)) &lt; 0.001: leftAns = 0 else: leftAns = math.sqrt(lssum-ln*(lsum/ln)*(lsum/ln)) if math.fabs(rssum-rn*(rsum/rn)*(rsum/rn)) &lt; 0.001: rightAns = 0 else: rightAns = math.sqrt(rssum-rn*(rsum/rn)*(rsum/rn)) nowAns = leftAns + rightAns if(nowAns&lt;minVariance): minVariance = nowAns attributeNum = (curAttributeNum[i]+curAttributeNum[i+1])/2 i = i + 1 return minVariance,attributeNumdef findBestFeatureToSplit(tree,dataMat): minVariance = sys.float_info.max attributeAns = 0 attributeNumAns = 0 for i in tree.attributes: curAttributeNum = [] for j in tree.items: curAttributeNum.append(dataMat[j][i]) j = j + 1 nowAns,attributeNum = calVariance(i,curAttributeNum,tree,dataMat) if(nowAns&lt;minVariance): minVariance = nowAns attributeAns = i attributeNumAns = attributeNum i = i + 1 return minVariance,attributeAns,attributeNumAnsdef calCurVariance(tree,dataMat): ssum = 0;sum=0 for i in tree.items: ssum = ssum + dataMat[i][-1]*dataMat[i][-1] sum = sum + dataMat[i][-1] return sqrt(ssum-len(tree.items)*(sum/len(tree.items))*(sum/len(tree.items))),sum/len(tree.items)def createTree(tree,dataMat,Top): curVariance,y = calCurVariance(tree,dataMat) if curVariance &lt; 1e-5: tree.num = y tree.isLeaf=1 return best,tree.label,tree.num = findBestFeatureToSplit(tree,dataMat) if math.fabs(best-curVariance) &lt; Top: tree.isLeaf=1 tree.num = 0 for i in tree.items: tree.num = tree.num + dataMat[i][-1] tree.num = tree.num/len(tree.items) return tree.leftNode = Node([],[],'',0,0) tree.rightNode = Node([],[],'',0,0) tree.leftNode.faNode = tree tree.rightNode.faNode = tree for i in tree.items: if (dataMat[i][tree.label] &lt;= tree.num): tree.leftNode.items.append(i) else: tree.rightNode.items.append(i) for i in tree.attributes: tree.leftNode.attributes.append(i) tree.rightNode.attributes.append(i) tree.leftNode.isLeft=1 createTree(tree.leftNode,dataMat,Top) createTree(tree.rightNode,dataMat,Top) returnLabel = [\"frequency\",\"angle\",\"xuanChang\",\"Free flow velocity\",\"Displacement thickness on suction side\"]n = 0def printTree(tree,dot): if tree.isLeaf == 1: dot.node(name=str(tree),label='y=' + str(tree.num)) else: dot.node(name=str(tree), label=str(Label[tree.label]), color='green') if tree.faNode!=None: dot.edge(str(tree.faNode),str(tree),label='&lt;='+str(tree.faNode.num) if tree.isLeft == 1 else \"&gt;\"+str(tree.faNode.num)) if tree.isLeaf == 0: printTree(tree.leftNode,dot) printTree(tree.rightNode,dot)def calErrorNoMerge(tree,lMat,rMat): error = 0 for item in lMat: val = item[-1] error = error + (val-tree.leftNode.num)*(val-tree.leftNode.num) for item in rMat: val = item[-1] error = error + (val-tree.rightNode.num)*(val-tree.rightNode.num) return errordef calErrorMerge(tree,testMat): error = 0 tree_num = (tree.leftNode.num+tree.rightNode.num)/2 for item in testMat: val = item[-1] error = error + (val-tree_num) * (val-tree_num) return errordef getMean(tree): if tree.leftNode.isLeaf==0: getMean(tree.leftNode) if tree.rightNode.isLeaf==0: getMean(tree.rightNode) if tree.rightNode.isLeaf == 1 and tree.rightNode.isLeaf == 1: tree.num = (tree.rightNode.num+tree.leftNode.num)/2 tree.isLeaf = 1 tree.leftNode=tree.rightNode=None returndef splitSet(idx,num,testMat): lset = [] rset = [] for item in testMat: if(item[idx]&lt;=num): lset.append(item) else: rset.append(item) return lset,rsetdef prune(tree,testMat): if(len(testMat)==0): getMean(tree) return lset,rset=splitSet(tree.label,tree.num,testMat) if tree.leftNode.isLeaf == 0: prune(tree.leftNode,lset) if tree.rightNode.isLeaf == 0: prune(tree.rightNode,rset) if tree.rightNode.isLeaf == 1 and tree.leftNode.isLeaf == 1: errorNoMerge = calErrorNoMerge(tree,lset,rset) errorMerge = calErrorMerge(tree,testMat) if(errorMerge&lt;errorNoMerge): tree.isLeaf = 1 tree.num = (tree.leftNode.num+tree.rightNode.num)/2 tree.leftNode = tree.rightNode = None print('merge') return returndef predict(tree,item): if(tree.isLeaf==1): return tree.num if item[tree.label]&lt;=tree.num:return predict(tree.leftNode,item) else: return predict(tree.rightNode,item)def fun(tree,testMat): #n = 1 sum = 0 L1 = [] L2 = [] for i in testMat: L1.append(predict(tree,i)) #print(\" 预测值:\",end=\"\"),print(predict(tree,i)) L2.append(i[-1]) #print(\" 实际值:\",end=\"\"),print(i[-1]) #n=n+1 i = 0 for i in range(0,len(L1)): sum = sum + (L1[i]-L2[i])*(L1[i]-L2[i]) sum = sum / len(L1) print(sum)testMat = loadFile1('/Users/seven7777777/QQDownload/test.dat')testMat2 = loadFile1('/Users/seven7777777/QQDownload/test2.dat')dot1 = Digraph(name=\"a\",comment=\"result1\",format=\"png\")dot2 = Digraph(name=\"b\",comment=\"result2\",format=\"png\")root,dataMat = init()createTree(root,dataMat,1)printTree(root,dot1)fun(root,testMat2)prune(root,testMat2)fun(root,testMat2)printTree(root,dot2)dot1.view(filename=\"result\", directory=\"/Users/seven7777777/QQDownload/\")dot2.view(filename=\"pruneResult\", directory=\"/Users/seven7777777/QQDownload/\")","link":"/2020/02/27/Cart%E6%A0%91%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"},{"title":"MacOS终端美化","text":"MacOS自带的terminal终端没有命令高亮等功能，且界面单一，我们可以通过增加背景图和iterm2+zsh+oh～my～zsh进行美化 优化效果 配置方法1.增加背景图打开终端的偏好设置进行更改 2.下载安装iterm2官网下载 https://www.iterm2.com 将安装包移动到应用程序中进行安装，后面可以使用这个终端也可以使用MacOS原来的终端 3.将shell切换为zsh1chsh -s /bin/zsh 4.安装oh~my~zsh1sh -c &quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; 5.安装字体库有些主题会要求特定的字体，Powerline字体: https://github.com/powerline/fonts 安装步骤如下： 12345678# clonegit clone https://github.com/powerline/fonts.git --depth=1# installcd fonts./install.sh# clean-up a bitcd ..rm -rf fonts 安装好之后，选择一款Powerline字体了：iterm2 -&gt; Preferences -&gt; Profiles -&gt; Text -&gt; Font -&gt; Change Font 6.修改主题配置打开家目录下的.zshrc文件,找到主题配置的地方进行修改 1ZSH_THEME=&quot;michelebologna&quot; 之后重启终端 命令高亮zsh-syntax-highlighting地址：https://github.com/zsh-users/zsh-syntax-highlighting 12git clone https://github.com/zsh-users/zsh-syntax-highlighting.gitecho &quot;source ${(q-)PWD}/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh&quot; &gt;&gt; ${ZDOTDIR:-$HOME}/.zshrc","link":"/2020/02/20/MacOS%E7%BB%88%E7%AB%AF%E7%BE%8E%E5%8C%96/"},{"title":"关联规则挖掘-apriori算法","text":"算法原理什么是关联规则？关联规则分析也称为购物篮分析，最早是为了发现超市销售数据库中不同的商品之间的关联关系。关联规则是反映一个事物与其他事物之间的关联性，若多个事物之间存在着某种关联关系，那么其中的一个事物就能通过其他事物预测到。举个例子：在超市中，经常发现啤酒和尿布经常在同一张购物清单上出现，那是因为买尿裤的一般是有孩子的中年男性，他们在超市给孩子买尿裤时，会买上几瓶啤酒来犒劳自己，于是啤酒和尿布就相互关联。 几个重要概念 频繁项集是指经常出现在一块的物品的集合 两个不相交的非空集合 $X,Y$ ,如果有 $X-&gt;Y$ ，就说 $X-&gt;Y$ 是一条关联规则，比如买尿布的同时会买上啤酒( $尿布-&gt;啤酒$ )。关联规则的强度由支持度($support$)和可信度($confidence$)来描述 项集 $X,Y$ 同时发生的概率称为关联规则 $X-&gt;Y$ 的支持度($support$)$$ support(X-&gt;Y)=P(X \\cup Y)=\\frac{X,Y同时发生的事件个数}{总事件数} $$ 项集 $X,Y$ 发生的前提下，项集 Y 发生的概率称为关联规则 $X-&gt;Y$ 的可信度($confidence$)$$confidence(X-&gt;Y)=\\frac{P(X\\cup Y)}{P(X)}=\\frac{X,Y同时发生的事件个数}{X发生的事件个数}$$Apriori算法的原理如果某个项集是频繁项集，那么它所有的子集也是频繁的，就是说如果一个项集是非频繁的，那么它的所有超集也是非频繁的。基于此，Apriori算法从单元素项集开始，通过组合满足最小支持度的项集来形成更大的集合。因此Apriori算法的步骤可描述如下：1234(1)生成频繁1项集(2)通过K频繁项集生成K+1候选项集(3)由K+1候选项集通过计算支持度生成K+1频繁项集(4)重复(2)(3)直至候选项集为空 通过算法描述可以发现，Apriori算法在计算支持度时需要扫描数据库多遍。Apriori算法实现 calCntForEveryComb()函数用来求每条记录里所有组合的出现次数，通过二进制枚举计算，这个函数把所有组合的出现次数记录在字典里，以后需要的时候不需要在扫描数据库，支持获取即可，由于list不能被hash所以可以先将它强转成str类型。1234567891011121314def calCntForEveryComb(dataMat): cnt = {} for List in dataMat: n = len(List) for i in range(1,1&lt;&lt;n): comb = [] for j in range(0,n): if((1&lt;&lt;j)&amp;i): comb.append(List[j]) comb = sorted(comb) hashVal = hash(str(comb)) if hashVal not in cnt:cnt[hashVal] = 1 else:cnt[hashVal]+=1 return cnt calL1() 函数计算出来的是频繁1项集1234567def calL1(dataMat,cnt,support): L = [] for itemList in dataMat: for item in itemList: if [item] not in L and cnt[hash(str([item]))] &gt;= support: L.append([item]) return L createL()通过候选项集算出频繁项集12345678def createL(preL,cnt,support): L = [] curC = aprioriGen(preL) for itemList in curC: itemList = sorted(itemList) if hash(str(itemList)) in cnt and cnt[hash(str(itemList))] &gt;= support: L.append(itemList) return L aprioriGen() 函数通过频繁k-1项集自交计算出k候选项集，只有经过排序后k-1项相同的k-1频繁项集才可以相交12345678910def aprioriGen(preL): k = len(preL[0]) L = [] for i in range(0,len(preL)): for j in range(i+1,len(preL)): L1 = sorted(list(preL[i])[:k-1]) L2 = sorted(list(preL[j])[:k-1]) if L1 == L2: L.append(list(set(preL[i]).union(set(preL[j])))) return L Apriori() 是算法的主体函数，不断的进行 频繁项集-&gt;候选项集-&gt;频繁项集 的过程来生成频繁项集 12345678def Apriori(dataMat,cnt,support): Lk = calL1(dataMat,cnt,support) L = [] while True: Lk = createL(Lk,cnt,support) if(len(Lk)==0):break L = L + Lk return L 计算出的部分频繁项集： 生成关联规则挖掘关联规则原理如下：若某条规则不满足可信度要求，则该规则的所有子集也不满足可信度要求。例如：{A,B,C}-&gt;{D}是不满足可信度的，那么 {A,B}-&gt;{C,D}也是不满足的。 getRules() 用来获取关联规则，如果当前是频繁2项集，就直接计算关联规则，否则通过挖掘规则的原理进行剪枝挖掘。12345678def getRules(L,cnt,conf): rules = [] for itemList in L: itemSet = set(itemList) H1 = [set([item]) for item in itemSet] if(len(itemSet)==2):findRules(itemSet,H1,cnt,conf,rules) else:prunedRules(itemSet,H1,cnt,conf,rules) return rules findRules() 通过可信度计算出关联规则，同时将目前关联规则的右件保存用于剪枝12345678def findRules(itemSet,H,cnt,conf,rules): prunedList = [] for item in H: curConf = 1.0*cnt[hash(str(sorted(list(itemSet))))]/cnt[hash(str(sorted(list(itemSet-item))))] if(curConf&gt;=conf): rules.append((itemSet-item,item,curConf)) prunedList.append(item) return prunedList prunedRules() 函数通过不断的增大频繁项集的右件，使得右件由小到大的变化，从而获取该频繁项集下的所有符合要求的关联规则1234567def prunedRules(itemSet,H,cnt,conf,rules): k = len(H[0]) if len(itemSet) &gt; k: nextH = findRules(itemSet,H,cnt,conf,rules) if(len(nextH)==0): return nextH = [set(item) for item in aprioriGen(nextH)] if(len(nextH)&gt;0) : prunedRules(itemSet,nextH,cnt,conf,rules) 生成的部分关联规则： 完整代码click here","link":"/2020/03/03/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98-apriori%E7%AE%97%E6%B3%95/"},{"title":"关联规则挖掘-fpGrowth算法","text":"算法原理什么是关联规则？见 Apriori算法 几个重要概念见 Apriori算法 fp-growth算法介绍 打开你的搜索引擎，输入一个单词或一部分，例如“我”，搜索引擎可能会去统计和“我”一块出现得多的词，然后返回给你。其实就是去找频繁项集，而且需要相当地高效，像Apriori那样的速度肯定是不行的了。 本文要介绍的是FP-growth算法，它被用于挖掘频繁项集，它把数据集存储为一个叫FP树的数据结构里，这样可以更高效地发现频繁项集或频繁项对。相比于Apriori对每个潜在的频繁项集都扫描数据集判定是否满足支持度，FP-growth算法只需要遍历两次数据库，因此它在大数据集上的速度显著优于Apriori。 FP即Frequent Pattern，FP树看上去就是一棵前缀树，根节点是空集，结点上是单个元素，保存了它在数据集中的出现次数，出现次数越多的元素越接近根。此外，结点之间通过链接（link）相连，只有相似元素会被连起来，连起来的元素又可以看成链表。同一个元素可以在FP树中多次出现，根据位置不同，对应着不同的频繁项集。可以为FP树设置最小支持度，过滤掉出现次数太少的元素。 举个例子假设现有数据集 instance id elements 0 r, z, h, j, p 1 z, y, x, w, v, u, t, s 2 z 3 r, x, n, o, s 4 y, r, x, z, q, t, p 5 y, z, x, e, q, s, t, m 首先求出频繁1项集，并且将他们在每条记录里按出现次数降序排序，并且把不满足支持度的删除，假设支持度为3，则处理之后的数据集为 instance id elements 0 z,r 1 z, x, y, s, t 2 z 3 x, s, r 4 z, x, y, r, t 5 z, x, y, s, t fp树的结构和字典树trie一样，知道字典树的同学可以把它当作是字典树，和字典树不同的是，对于每一个元素，fp树有一个头指针，这个在后面会介绍到。 开始插入第一行记录，每经过一次节点，对应节点的值就加一同时头指针传递指向这个节点 插入第二行记录 全部插入之后 头指针用于生成条件模式基，通过头指针从一个叶节点到另一个叶节点开始向根部求解得到的前缀就是条件模式基 例如t的条件模式基： id elements count 0 z,x,y,s, 2 1 z,x,y,r 1 根据支持度删除掉不满足条件的项 id elements count 0 z,x,y 2 1 z,x,y 1 根据条件模式基创建条件模式树 通过不断的构造条件模式树得到频繁项集，这是通过t的条件模式树可以得到频繁项集{t},{t,y},{t,y,x},{t,y,x,z} fp-growth算法实现 树的存储形式采用对象的方式1234567class Node: def __init__(self,name,count,parent): self.name = name #节点名称 self.count = count #经过节点的次数 self.parent = parent #父节点 self.child = {} #子节点 self.nodeLink = None #头指针 createInitDateSet() 用于生成初始项集12345678def createInitDateSet(dataMat): dataDic = {} for itemList in dataMat: if frozenset(itemList) not in dataDic: dataDic[frozenset(itemList)] = 1 else: dataDic[frozenset(itemList)] += 1 return dataDic createTree() 用于创建fp树,首先获取头指针列表，删除不满足支持度的项,将所有记录里的项按出现次数降序排列，然后将所有记录插入到树中。12345678910111213141516171819202122232425262728293031def createTree(dataSet,sup): headTable = {} for itemList in dataSet: for item in itemList: headTable[item] = headTable.get(item,0) + dataSet.get(frozenset(itemList)) #print(len(headTable)) validKeys = [] for key in headTable.keys(): if(headTable[key]&lt;sup): validKeys.append(key) for key in validKeys: del headTable[key] #print(len(headTable)) freqItemSet = set(headTable.keys()) if(len(freqItemSet) == 0) : return None,None for key in headTable.keys(): headTable[key] = [headTable[key],None] root = Node('root',0,None) for itemDic,count in dataSet.items(): curItemDic = {} for item in itemDic: if item in freqItemSet: curItemDic[item] = headTable[item][0] if len(curItemDic) &gt; 0: sortedItems = [v[0] for v in sorted(curItemDic.items(), key=lambda x: x[1], reverse=True)] insertIntoTree(sortedItems,root,headTable,count) return root,headTable insertIntoTree() 用于将记录插入树中，插入过程和字典树的插入过程相同，首先判断该节点是否有这项，没有的话创建节点，有的话增加数目，之后递归插入。123456789101112def insertIntoTree(itemList,tree,headTable,count): item = itemList[0] if item in tree.child: tree.child[item].count += count else: tree.child[item] = Node(item,count,tree) if headTable[item][1] == None: headTable[item][1] = tree.child[item] else: updateHead(headTable[item][1],tree.child[item]) if len(itemList[1:])&gt;0: insertIntoTree(itemList[1:],tree.child[item],headTable,count) updateHead() 用于在插入的时候更新头指针，新的节点增加，需要使得头指针指向这个节点。1234def updateHead(headNode,treeNode): while(headNode.nodeLink != None): headNode = headNode.nodeLink headNode.nodeLink = treeNode createConTree() 用于创建条件模式树，首先获取每个头指针的条件模式集，同时保存频繁项集，创建条件模式树，如果条件模式树不为空，则递归求解12345678910111213def createConTree(FPtree,headTable,sup,preFix,freqItemList): #print(headTable) itemList = [v[0] for v in sorted(headTable.items(), key=lambda x: x[1][0])] for beginItem in itemList: newFreqSet = copy.deepcopy(preFix) newFreqSet.add(beginItem) freqItemList.append(newFreqSet) condBasePath = findPreFix(beginItem,headTable[beginItem][1]) #print(condBasePath) conTree,conHead = createTree(condBasePath,sup) if conTree != None: createConTree(conTree,conHead,sup,newFreqSet,freqItemList) findPreFix() 用于求解条件模式基，随着头指针的移动，递归求解出所有的条件模式基123456789def findPreFix(beginItem,LinkNode): condBasePath = {} while LinkNode != None: preFix = [] findParent(LinkNode,preFix) if len(preFix)&gt;1: condBasePath[frozenset(preFix[1:])] = LinkNode.count LinkNode = LinkNode.nodeLink return condBasePath findParent() 可以递归的向上求解前缀1234def findParent(curNode,preFix): if curNode.parent != None: preFix.append(curNode.name) findParent(curNode.parent,preFix) 求解出的部分频繁项集生成关联规则步骤与Apriori相同 见Apriori 完整代码click here","link":"/2020/03/03/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98-fpGrowth%E7%AE%97%E6%B3%95/"},{"title":"在腾讯云服务器上部署hexo博客","text":"如何在腾讯云服务器上部署hexo博客本人在搭建过程中遇到了很多问题，因此写一个教程贴来总结一下 前期准备 本地环境 macOS Catalina 10.15 git node.js hexo 服务器端 腾讯云服务器 CentOS 7.6 64 Bit git node.js Nginx 服务器端部署 1.安装git1yum install git 2.安装node.js 1curl --silent --location https://rpm.nodesource.com/setup_5.x | bash - 使用 git --version 和 node --version 查看版本号 3.安装Nginx 1yuminstall nginx 使用 nginx -v查看版本号 4.配置Nginx 1vim /etc/nginx/nginx.conf 进行以下修改 1234567891011121314151617181920server { listen 80 default_server; listen [::]:80 default_server; server_name www.seven7.xyz seven7.xyz; #域名 root /home/hexo; #网站的根目录 自行创建 #以下不作修改 # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / { } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } 配置完成后保存退出，使用nginx -t查看配置是否有错误。查看运行状态：systemctl status nginx，显示running表示成功运行 5.创建git用户123useradd gitchmod chmod 740 /etc/sudoersvim /etc/sudoers 在sudoers中找到下面内容 12## Allow root to run any commands anywhereroot ALL=(ALL) ALL 并且在下面增加 1git ALL=(ALL) ALL 修改权限 1chmod 400 /etc/sudoers 使用su git切换到git用户 1234567891011# 切换到git用户目录cd /home/git# 创建.ssh文件夹mkdir ~/.ssh# 创建authorized_keys文件并编辑vim ~/.ssh/authorized_keys# 如果你还没有生成公钥，那么首先在本地电脑中执行 cat ~/.ssh/id_rsa.pub | pbcopy生成公钥# 再将公钥复制粘贴到authorized_keys# 保存关闭authorized_keys后，修改相应权限chmod 600 ~/.ssh/authorized_keyschmod 700 ~/.ssh 本地测试是否可以进行免密码登陆 1ssh -v git@服务器IP地址 6.建立git裸库1234# 回到git目录cd /home/git# 使用git用户创建git裸仓库，以blog.git为例git init --bare blog.git 使用下列命令修改用户组权限 12sudo chown git:git -R /home/hexosudo chown git:git -R /home/git/blog.git 7.使用git-hooks同步网站根目录1vim ~/blog.git/hooks/post-receive 在里面输入下列内容 12#!/bin/shgit --work-tree=/home/hexo --git-dir=/home/git/blog.git checkout -f 保存退出后执行 chmod +x post-receive修改权限 到这里服务器端就已经配置完成了 本地端配置 打开hexo根目录下的_config.yml文件，进行下列配置 1234deploy: type: git repo: git@你的服务器IP:/home/git/blog.git branch: master 然后发布 1hexo clean &amp;&amp; hexo g &amp;&amp; hexo d","link":"/2020/02/06/%E5%9C%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E9%83%A8%E7%BD%B2hexo%E5%8D%9A%E5%AE%A2/"},{"title":"爬虫入门之从当当网爬取图片","text":"1234567891011121314151617181920212223242526272829303132import reimport urllib.request#获取html页面def get_html(url): page = urllib.request.urlopen(url) html_a = page.read() #根据网页编码设置编码方式 return html_a.decode('gbk')def get_img(html): #封装正则表达式对象 reg = r'http://[^\\s]*?\\.jpg' imgre = re.compile(reg) #获取所有的图片url imglist = imgre.findall(html) x = 0 path = '/Users/seven7777777/code/' for imgurl in imglist: #将url以图片形式存储到本地 urllib.request.urlretrieve(imgurl, '{0}{1}.jpg'.format(path, x)) x = x + 1 return imglistif __name__ == '__main__': url = 'http://www.dangdang.com' #get_html(url) print (get_img(get_html(url)))","link":"/2020/02/20/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B9%8B%E4%BB%8E%E5%BD%93%E5%BD%93%E7%BD%91%E7%88%AC%E5%8F%96%E5%9B%BE%E7%89%87/"},{"title":"神经网络作业一","text":"题目 将数据映射到[0-1] department 映射前 映射后 sales 0 systems 0.33 marketing 0.67 secretary 1 status 映射前 映射后 senior 0 junior 1 age 映射前 映射后 21···25 0 26···30 0.25 31···35 0.5 36···40 0.75 41···45 1 salary 映射前 映射后 26K···30K 0 31K···35K 0.125 36K···40K 0.250 41K···45K 0.375 46K···50K 0.500 51K···55K 0.625 56K···60K 0.750 61K···65K 0.875 66K···70K 1 构建前馈神经网络 $w_{14} = 0.07000$ $w_{15} = 0.23000$ $w_{16} = 0.30000$ $w_{17} = 0.44000$$w_{24} = 0.23000$ $w_{25} = 0.40000$ $w_{26} = 0.42000$ $w_{27} = 0.37000$$w_{34} = 0.27000$ $w_{35} = 0.40000$ $w_{36} = 0.03000$ $w_{37} = 0.09000$$w_{48} = -0.10000$ $w_{58} = -0.49000$ $w_{68} = 0.16000$ $w_{78} = 0.47000$ $\\theta_4 = 0.12$ $\\theta_5 = -0.1$ $\\theta_6 = 0.29$ $\\theta_7 = -0.29$ $\\theta_8 = 0.17$ 测试数据为 (sales,senior,31···35,46K···50K)映射之后为(0,0,0.5,0.5)其中输入为(0,0.5,0.5)输出为0 计算各个节点的输出$I_4 = w_{14}*x_1+w_{24}*x_2+w_{34}*x_3 + heta_4 = 0.370$$O_4 = \\displaystyle\\frac{1}{1+e^-{I_4}}=0.591$ $I_5 = w_{15}*x_1+w_{25}*x_2+w_{35}*x_3 + heta_5 = 0.300$$O_5 = \\displaystyle\\frac{1}{1+e^-{I_5}}=0.574$ $I_6 = w_{16}*x_1+w_{26}*x_2+w_{36}*x_3 + heta_6 = 0.515$$O_6 = \\displaystyle\\frac{1}{1+e^-{I_6}}=0.626$ $I_7 = w_{17}*x_1+w_{27}*x_2+w_{37}*x_3 + heta_7 = -0.060$$O_7 = \\displaystyle\\frac{1}{1+e^-{I_7}}=0.485$ $I_7 = w_{48}*O_4+w_58*O_5+w_{68}*O_6 + w_{78}*O_7 + \\theta_8 = 0.398$$O_8 = \\displaystyle\\frac{1}{1+e^-I_8}=0.598$ 计算误差$Err_8 = O_8(1-O_8)(T_8-O_8)=-0.144$ $Err_7 = O_7(1-O_7)(\\sum_kErr_kw_{jk})=O_7(1-O_7)(Err_8)(W_{78})=-0.017$ $Err_6 = O_6(1-O_6)(\\sum_kErr_kw_{jk})=O_6(1-O_6)(Err_8)(W_{68})=-0.005$ $Err_5 = O_5(1-O_5)(\\sum_kErr_kw_{jk})=O_5(1-O_5)(Err_8)(W_{58})=0.017$ $Err_4 = O_4(1-O_4)(\\sum_kErr_kw_{jk})=O_4(1-O_4)(Err_8)(W_{48})=0.003$ 更新权值和偏置值 权值 $w_{14} = w_{14}+(l)*Err_4*O_1=0.070$ $w_{15} = w_{15}+(l)*Err_5*O_1=0.230$ $w_{16} = w_{16}+(l)*Err_6*O_1=0.300$ $w_{17} = w_{17}+(l)*Err_7*O_1=0.440$ $w_{24} = w_{24}+(l)*Err_4*O_2=0.232$ $w_{25} = w_{25}+(l)*Err_5*O_2=0.408$ $w_{26} = w_{26}+(l)*Err_6*O_2=0.418$ $w_{27} = w_{27}+(l)*Err_7*O_2=0.362$ $w_{34} = w_{34}+(l)*Err_4*O_3=0.272$ $w_{35} = w_{35}+(l)*Err_5*O_3=0.408$ $w_{36} = w_{36}+(l)*Err_6*O_3=0.028$ $w_{37} = w_{37}+(l)*Err_7*O_3=0.082$ $w_{48} = w_{48}+(l)*Err_8*O_4=-0.177$ $w_{58} = w_{58}+(l)*Err_8*O_5=-0.564$ $w_{68} = w_{68}+(l)*Err_8*O_6=0.079$ $w_{78} = w_{78}+(l)*Err_8*O_7=0.407$ $w_{48} = w_{48}+(l)*Err_8*O_4=-0.177$ $w_{58} = w_{58}+(l)*Err_8*O_5=-0.564$ $w_{68} = w_{68}+(l)*Err_8*O_6=0.079$ $w_{78} = w_{78}+(l)*Err_8*O_7=0.407$ $w_{48} = w_{48}+(l)*Err_8*O_4=-0.177$ $w_{58} = w_{58}+(l)*Err_8*O_5=-0.564$ $w_{68} = w_{68}+(l)*Err_8*O_6=0.079$ $w_{78} = w_{78}+(l)*Err_8*O_7=0.407$ $w_{48} = w_{48}+(l)*Err_8*O_4=-0.177$ $w_{58} = w_{58}+(l)*Err_8*O_5=-0.564$ $w_{68} = w_{68}+(l)*Err_8*O_6=0.079$ $w_{78} = w_{78}+(l)*Err_8*O_7=0.407$ $w_{48} = w_{48}+(l)*Err_8*O_4=-0.177$ $w_{58} = w_{58}+(l)*Err_8*O_5=-0.564$ $w_{68} = w_{68}+(l)*Err_8*O_6=0.079$ $w_{78} = w_{78}+(l)*Err_8*O_7=0.407$ 偏置值 $\\theta_4 = \\theta_4+(l)*Err_4=0.123$ $\\theta_5 = \\theta_5+(l)*Err_5=-0.084$ $\\theta_6 = \\theta_6+(l)*Err_6=0.285$ $\\theta_7 = \\theta_7+(l)*Err_7=-0.305$ $\\theta_8 = \\theta_8+(l)*Err_8=0.041$ 完整代码click here","link":"/2020/03/05/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%9A%E4%B8%80/"},{"title":"贝叶斯分类器","text":"地震预报是比较困难的一个课题,可以根据地震与生物异常反应之间的联系来进行研究。根据历史记录的统计,地震前一周内出现生物异常反应的概率为50%,而一周内没有发生地震但也出现了生物异常反应的概率为 10%。假设某 一个地区属于地震高发区,发生地震的概率为 20%。问:如果某日观察到明显的生物异常反应现象,是否应当预报一周内将发生地震? 设发生地震为$w_1$，不发生地震为$w_2$，当前出现的状态(出现了生物异常)为$x$。由题意可得$P(x|w_1)=0.5,P(x|w_2)=0.1,P(w_1)=0.2,P(w_2)=0.8$ 当前情况下$w_1$的概率为$P(w_1|x)=\\displaystyle\\frac{P(x|w_1)P(w_1)}{P(x)}$ 当前情况下$w_2$的概率为$P(w_2|x)=\\displaystyle\\frac{P(x|w_2)P(w_2)}{P(x)}$ $P(x)=\\sum_{j=1}^2P(x|w_j)P(w_j)=0.18$$P(w_1|x)=\\displaystyle\\frac{P(x|w_1)P(w_1)}{P(x)}=0.5556$$P(w_2|x)=\\displaystyle\\frac{P(x|w_2)P(w_2)}{P(x)}=0.4444$所以$P(w_1|x)=0.5556&gt;P(w_2|x)=0.4444$，故应当预报一周内将发生地震 对于上例中的地震预报问题,假设预报一周内发生地震,可以预先组织抗震 救灾,由此带来的防灾成本会有 2500 万元,而当地震确实发生时,由于地震造 成的直接损失会有 1000 万元;假设不预报将发生地震而地震又发生了,造成的 损失会达到 5000 万元。请问在观察到明显的生物异常反应后,是否应当预报一周内将发生地震? 根据题意可得：实际发生地震且预报发生地震的损失$\\lambda_1^1=3500$实际发生地震却不预报发生地震的损失$\\lambda_1^2=5000$实际不发生地震却预报发生地震的损失$\\lambda_2^1=2500$实际不发生地震且不预报发生地震的损失$\\lambda_2^2=0$ $R(\\alpha_1|X)$$=\\sum_{j=1}^2\\lambda_j^1P(w_j|x)$$=\\lambda_1^1P(w_1|x)+\\lambda_2^1P(w_2|x)$$=3500*\\displaystyle\\frac{P(x|w_1)P(w_1)}{P(x)}+2500*\\displaystyle\\frac{P(x|w_2)P(w_2)}{P(x)}$$=\\displaystyle\\frac{550}{P(x)}=3055.6$ $R(\\alpha_2|X)$$=\\sum_{j=1}^2\\lambda_j^2P(w_j|x)$$=\\lambda_1^2P(w_1|x)+\\lambda_2^2P(w_2|x)$$=5000*\\displaystyle\\frac{P(x|w_1)P(w_1)}{P(x)}$$=\\displaystyle\\frac{500}{P(x)}=2777.8$ 故$R(\\alpha_1|X)=3055.6&gt;R(\\alpha_2|X)=2777.8$，所以不应当预报一周内将发生地震","link":"/2020/02/19/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/"},{"title":"资源分享","text":"免费图床catbox、免注册、最大支持200M click here","link":"/2020/02/28/%E8%B5%84%E6%BA%90%E5%88%86%E4%BA%AB/"},{"title":"神经网络作业二","text":"题意考虑以下的二类训练样本集 Instance Feature vector Output label 1 (0,0) + 2 (1,0) + 3 (0,1) - 4 (-1,0) - 5 (1,-1) - 对此训练样本集，我们需要训练一个三层神经网络（输入层、单隐层、输出层），其中单隐层的单元（神经元）数目设为2，激活函数（activation function）为Sigmoid函数： （1）在二维坐标系中画出这5个训练样本点，并讨论此训练样本集是否线性可分。（2）试分析将Sigmoid激活函数换成线性函数的缺陷。（3）令初始化参数全部为0，试运用前馈（feedforward）算法计算在初始化参数下此三层神经网络的输出；然后运用反向传播（backpropagation）算法，计算代价函数对所有参数的偏导数，并讨论将初始化参数全部设为0所带来的问题。（4）试给出一个神经网络（画出架构图，并写出激活函数及其对应的参数），使此训练样本集的5个训练样本点都可以被正确分类。 答案**(1)** 作出的图如下，显然不存在一条直线可以将样本实现正确的分类。**(2)** 如果激活函数换成线性函数，那么无论神经网络有多少层，输出都是输入的线性组合，与没有隐藏层的效果相当，就成了最原始的感知器了，与不使用激活函数、直接使用逻辑回归没有区别。**(3)** 由题意知$w_{i,j}=0,\\theta_i=0$使用第一条数据进行计算,输入$x_1=0,x_2=0,y=0$ 正向传播$I_3= x_1*w_{13}+x_2*w_{23}+\\theta_3=0*0+0*0+0=0$ $O_3 = sigmoid(I_3) = 0.5$ 同理$I_4 =0$$O_4 = 0.5$$I_5 =0$$O_5 = 0.5$ 反向传播计算误差对各参数的偏导数$E=\\frac{1}{2}*(target_5-O_5)^2$ $\\displaystyle\\frac{\\partial E}{\\partial w_{35}}=\\displaystyle\\frac{\\partial I_5}{\\partial w_{35}}*\\displaystyle\\frac{\\partial O_5}{\\partial I_5}*\\displaystyle\\frac{\\partial E}{\\partial O_5}$ $\\displaystyle\\frac{\\partial E}{\\partial O_5}=-(target_5-O_5)$ $\\displaystyle\\frac{\\partial O_5}{\\partial I_5}=\\displaystyle\\frac{\\partial \\frac{1}{1+e^{-I_5}}}{\\partial I_5}=O_5*(1-O_5)$ $\\displaystyle\\frac{\\partial I_5}{\\partial w_{35}}=\\displaystyle\\frac{\\partial (w_{35}*O_3+w_{45}*O_4+\\theta_5*1)}{\\partial w_{35}}=O_3$ $\\therefore \\displaystyle\\frac{\\partial E}{\\partial w_{35}}=-O_5*(1-O_5)*(target_5-O_5)*O_3=-0.0625$ $\\displaystyle\\frac{\\partial E}{\\partial \\theta_5}=\\displaystyle\\frac{\\partial I_5}{\\partial \\theta_5}*\\displaystyle\\frac{\\partial O_5}{\\partial I_5}*\\displaystyle\\frac{\\partial E}{\\partial O_5}$ $=\\displaystyle\\frac{\\partial I_5}{\\partial \\theta_5}*\\displaystyle\\frac{\\partial O_5}{\\partial I_5}*\\displaystyle\\frac{\\partial (w_{35}*O_3+w_{45}*O_4+\\theta_5*1)}{\\partial \\theta_5}$ $=\\displaystyle\\frac{\\partial I_5}{\\partial \\theta_5}*\\displaystyle\\frac{\\partial O_5}{\\partial I_5}$ $=-O_5*(1-O_5)*(target_5-O_5)=-0.125$ 同理 $\\displaystyle\\frac{\\partial E}{\\partial w_{45}}=-0.0625$ $\\displaystyle\\frac{\\partial E}{\\partial w_{13}}=0$ $\\displaystyle\\frac{\\partial E}{\\partial w_{14}}=0$ $\\displaystyle\\frac{\\partial E}{\\partial w_{23}}=0$ $\\displaystyle\\frac{\\partial E}{\\partial w_{24}}=0$ $\\displaystyle\\frac{\\partial E}{\\partial \\theta_4}=0$ $\\displaystyle\\frac{\\partial E}{\\partial \\theta_3}=0$ 初始化参数全部设为0所带来的问题如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值，那么隐藏神经元对输出单元的影响也是相同的，通过反向传播梯度下降法进行计算时，会得到同样的梯度大小，所以无论设置多少个隐藏单元，其最终的影响都是相同的。同理，也不能初始化所有的参数都为同一个非0的数。因此，要随机化初始参数，以打破对称性。 (4) 经训练后得到参数如下: $w_{ 1 3 }= -7.204$ $w_{ 2 3 }= 7.898$ $\\theta_ 3 = 3.295$ $w_{ 1 4 }= 0.139$ $w_{ 2 4 }= 6.273$ $\\theta_4 = -2.906$ $w_{ 3 5 }= -9.242$ $w_{ 4 5 }= 8.832$ $\\theta_5 = 3.967$ 预测结果:通过结果来看，5个训练样本点都可以被正确分类 **完整代码**click here","link":"/2020/03/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%9A%E4%BA%8C/"}],"tags":[{"name":"MAC","slug":"MAC","link":"/tags/MAC/"},{"name":"决策树","slug":"决策树","link":"/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"name":"关联规则挖掘","slug":"关联规则挖掘","link":"/tags/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"爬虫","slug":"爬虫","link":"/tags/%E7%88%AC%E8%99%AB/"},{"name":"神经网络","slug":"神经网络","link":"/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"不定时更新","slug":"不定时更新","link":"/tags/%E4%B8%8D%E5%AE%9A%E6%97%B6%E6%9B%B4%E6%96%B0/"}],"categories":[{"name":"教程","slug":"教程","link":"/categories/%E6%95%99%E7%A8%8B/"},{"name":"数据挖掘","slug":"数据挖掘","link":"/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"学习笔记","slug":"学习笔记","link":"/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"资源总结","slug":"资源总结","link":"/categories/%E8%B5%84%E6%BA%90%E6%80%BB%E7%BB%93/"}]}