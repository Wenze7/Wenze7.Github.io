{"pages":[{"title":"tags","text":"","link":"/tags/index.html"},{"title":"About","text":"这个人很懒，什么都没留下","link":"/about/index.html"}],"posts":[{"title":"MAC和iphone之间无法使用接力的解决办法","text":"最近我的iphone和MAC之间不能使用接力了，费了老大劲才找到解决办法 确保按照官方指示开启所要求的功能 所有设备均使用同一 Apple ID 登录 iCloud。 所有设备均已开启蓝牙。 所有 Mac、iPhone、iPad 或 iPod touch 均已开启 Wi-Fi。 所有设备均已开启接力： 在 Mac 上，选取左上角的苹果菜单 -“系统偏好设置”-“通用”。选中“允许在这台 Mac 和 iCloud 设备之间使用接力”。 在 iPhone、iPad 或 iPod touch 上，前往“设置”-“通用”-“接力”，然后开启接力。 （支持接力的应用包括“邮件”、“地图”、“Safari 浏览器”、“提醒事项”、“日历”、“通讯录”、“Pages 文稿”、“Numbers 表格”、“Keynote 讲演”，以及众多第三方应用。） 上述操作进行后仍不行，请关闭SIP(系统完整性保护) 终端输入csrutil status查看SIP状态，如果是enable则是开启，是disable是关闭。 重启MAC，立即在键盘上按住 Command ⌘ + R，直到看到 Apple 标志，进入Recovery模式。 在上方的菜单栏点击「实用工具」选择「终端」。 在终端中，输入csrutil enable后回车。 点击菜单栏  标志，选择「重新启动」。这样接力就可以使用了。 如果要开启SIP，则重新进入Recovery模式，打开终端，输入csrutil enable即可。","link":"/2020/03/21/MAC%E5%92%8Ciphone%E4%B9%8B%E9%97%B4%E6%97%A0%E6%B3%95%E4%BD%BF%E7%94%A8%E6%8E%A5%E5%8A%9B%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"},{"title":"MacOS终端美化","text":"MacOS自带的terminal终端没有命令高亮等功能，且界面单一，我们可以通过增加背景图和iterm2+zsh+oh～my～zsh进行美化 优化效果 配置方法1.增加背景图打开终端的偏好设置进行更改 2.下载安装iterm2官网下载 https://www.iterm2.com 将安装包移动到应用程序中进行安装，后面可以使用这个终端也可以使用MacOS原来的终端 3.将shell切换为zsh1chsh -s /bin/zsh 4.安装oh~my~zsh1sh -c &quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; 5.安装字体库有些主题会要求特定的字体，Powerline字体: https://github.com/powerline/fonts 安装步骤如下： 12345678# clonegit clone https://github.com/powerline/fonts.git --depth=1# installcd fonts./install.sh# clean-up a bitcd ..rm -rf fonts 安装好之后，选择一款Powerline字体了：iterm2 -&gt; Preferences -&gt; Profiles -&gt; Text -&gt; Font -&gt; Change Font 6.修改主题配置打开家目录下的.zshrc文件,找到主题配置的地方进行修改 1ZSH_THEME=&quot;michelebologna&quot; 之后重启终端 命令高亮zsh-syntax-highlighting地址：https://github.com/zsh-users/zsh-syntax-highlighting 12git clone https://github.com/zsh-users/zsh-syntax-highlighting.gitecho &quot;source ${(q-)PWD}/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh&quot; &gt;&gt; ${ZDOTDIR:-$HOME}/.zshrc","link":"/2020/02/20/MacOS%E7%BB%88%E7%AB%AF%E7%BE%8E%E5%8C%96/"},{"title":"Cart树的原理及实现","text":"CART在回归预测上的原理及实现1.CART简介CART是指分类回归树，Classfication And Regression Tree，缩写为CART,CART算法采用二分递归分割的技术将当前样本集分为两个子样本集，使得生成的每个非叶子节点都有两个分支。所以CART的结构是二叉树，CART可以处理连续型变量和离散型变量，利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。 如果待预测分类是离散型数据，则CART生成分类决策树。 如果待预测分类是连续性数据，则CART生成回归决策树。 2.CART做回归预测在处理连续值时，CART采取回归的方式进行预测，假设X与Y分别是输入变量和输出变量，并且Y的取值是连续的，设训练集为D。$$D={(x_1,y_1),(x_2,y_2),(x_3,Y_3),…,(x_n,y_n)}$$CART的度量目标是，选取一个特征A和其对应的划分点s来划分数据集，使得划分出来的两个数据集的均方误差最小,不断的进行上述操作，当划分之后的均方差和划分之前的均方差之差的绝对值小于一个特定的阀值时，停止划分。$$min[min\\sum_{x_i\\in D_1(A,s)}(y_i-c_1)+min\\sum_{x_i\\in D_2(A,s)}(y_i-c_2)]$$其中$c_1$为$D_1$的样本输出均值，$c_2$为$D_2$的样本输出均值。在使用测试集时，采用均方误差作为预测误差。$$MSE=\\frac{1}{n}\\sum_{i=1}^n(predicted_i−label)^2$$其中$predicted_i$为预测值，$label$为实际预测值。 3.题目内容使用UCI公开数据集airfoil_self_noise（翼型自噪声），该数据集是NACA在2014年发布的一组关于0012翼型机在不同风洞速度和角度的数据，包含6个属性，分别是频率、角度、炫长、自由流速度、吸力侧位移厚度以及输出的y值低压声等级。该数据统计信息如下： 数据 统计值 example 1503 Training set 1200 Test set 303 Range of y 103.38-140.987 attributes 6 4.具体实现 结点的存储采用对象的方式进行，相比与字典的存储方式，虽代码量大但结构清晰。12345678910111213141516class Node: faNode = None #父节点 leftNode = None #左分支 rightNode = None #右分支 items = [] #划分到当前分支的数据集 attributes = [] #特征 label = 0 #划分这个节点的特征 num = 0 #划分的值或者输出的值 isLeaf = 0 #判断当前节点是否为叶节点 isLeft = 0 #判断当前节点是否为左分支，主要用于可视化 def __init__(self,items,attributes,label,num,isLeaf,): self.items = items self.attributes = attributes self.label = label self.num = num self.isLeaf = isLeaf 树的创建1234找到最佳的待切分特征：如果该节点不能再分，将该节点存为叶节点,执行二元切分在右子树调用 createTree() 方法在左子树调用 createTree() 方法 123456789101112131415161718192021222324252627282930313233343536373839404142def calCurVariance(tree,dataMat): ssum = 0;sum=0 for i in tree.items: ssum = ssum + dataMat[i][-1]*dataMat[i][-1] sum = sum + dataMat[i][-1] return sqrt(ssum-len(tree.items)*(sum/len(tree.items))*(sum/len(tree.items))),sum/len(tree.items)def createTree(tree,dataMat,Top): curVariance,y = calCurVariance(tree,dataMat) if curVariance &lt; 1e-5: tree.num = y tree.isLeaf=1 return best,tree.label,tree.num = findBestFeatureToSplit(tree,dataMat) if math.fabs(best-curVariance) &lt; Top: tree.isLeaf=1 tree.num = 0 for i in tree.items: tree.num = tree.num + dataMat[i][-1] tree.num = tree.num/len(tree.items) return tree.leftNode = Node([],[],'',0,0) tree.rightNode = Node([],[],'',0,0) tree.leftNode.faNode = tree tree.rightNode.faNode = tree for i in tree.items: if (dataMat[i][tree.label] &lt;= tree.num): tree.leftNode.items.append(i) else: tree.rightNode.items.append(i) for i in tree.attributes: tree.leftNode.attributes.append(i) tree.rightNode.attributes.append(i) tree.leftNode.isLeft=1 createTree(tree.leftNode,dataMat,Top) createTree(tree.rightNode,dataMat,Top) return calCurVariance() 用于计算当前节点内部的均方差。 createTree() 用于创建树节点，这个函数有3个参数 123tree : 当前节点dataMat : 数据集矩阵Top : 阀值 函数先计算当前节点内部的均方差，当均方差接近于0时，说明这个里面的数据集纯度已经很高，不再进行划分。 12345curVariance,y = calCurVariance(tree,dataMat)if curVariance &lt; 1e-5: tree.num = y # y是输出，即样本的输出均值 tree.isLeaf=1 return 然后找到最优的划分特征即特征点，判断是否划分 12345678best,tree.label,tree.num = findBestFeatureToSplit(tree,dataMat)if math.fabs(best-curVariance) &lt; Top: tree.isLeaf=1 tree.num = 0 for i in tree.items: tree.num = tree.num + dataMat[i][-1] tree.num = tree.num/len(tree.items) return 如果进行划分的话，就创建左右分支并且进行递归 12345678910111213141516tree.leftNode = Node([],[],'',0,0)tree.rightNode = Node([],[],'',0,0)tree.leftNode.faNode = treetree.rightNode.faNode = treefor i in tree.items:if (dataMat[i][tree.label] &lt;= tree.num): tree.leftNode.items.append(i)else: tree.rightNode.items.append(i)for i in tree.attributes:tree.leftNode.attributes.append(i)tree.rightNode.attributes.append(i)tree.leftNode.isLeft=1createTree(tree.leftNode,dataMat,Top)createTree(tree.rightNode,dataMat,Top) 树的划分 findBestFeatureToSplit() 用于寻找最优划分的特征和划分点，对于连续型的特征，可以把特征值从小到大排序，然后取两个相近特征值的均值作为候选划分点。假设特征值可以取$a_1,a_2,a_3,a_4,a_5,a_6$，那么可以取 $\\frac{a_1+a_2}{2},\\frac{a_2+a_3}{2},\\frac{a_3+a_4}{2},\\frac{a_4+a_5}{2},\\frac{a_5+a_6}{2}$ 这六个值当作候选的划分点，特征点小于或等于划分点的划分到左分支，大于特征点的划分到右分支。 12345678910111213141516def findBestFeatureToSplit(tree,dataMat): minVariance = sys.float_info.max attributeAns = 0 attributeNumAns = 0 for i in tree.attributes: curAttributeNum = [] for j in tree.items: curAttributeNum.append(dataMat[j][i]) j = j + 1 nowAns,attributeNum = calVariance(i,curAttributeNum,tree,dataMat) if(nowAns&lt;minVariance): minVariance = nowAns attributeAns = i attributeNumAns = attributeNum i = i + 1 return minVariance,attributeAns,attributeNumAns 对于每一个特征使用calVariance()函数找到其最佳的特征点，并与当前最优结果进行比较。 123456789101112131415161718192021222324252627282930313233343536373839404142434445def calVariance(curAttribute,curAttributeNum,tree,dataMat): attributeNum = 0 minVariance = sys.float_info.max lsum = 0;lssum = 0;ln = 0;rsum = 0;rssum = 0;rn = 0; class Pair: a = 0 b = 0 def __init__(self,a,b): self.a = a self.b = b def __lt__(self, other): return self.a &lt; other.a Pairs = [] for i in tree.items: Pairs.append(Pair(dataMat[i][curAttribute],dataMat[i][-1])) rn = rn + 1 rssum = rssum + dataMat[i][-1]*dataMat[i][-1] rsum = rsum + dataMat[i][-1] Pairs=sort(Pairs) curAttributeNum = sort(list(set(curAttributeNum))) i = 0 j = 0 while i &lt; len(curAttributeNum)-1: while Pairs[j].a &lt;= (curAttributeNum[i]+curAttributeNum[i+1])/2: ln = ln + 1 lsum = lsum + Pairs[j].b lssum = lssum + Pairs[j].b*Pairs[j].b rn = rn - 1 rsum = rsum - Pairs[j].b rssum = rssum - Pairs[j].b*Pairs[j].b j = j + 1 if math.fabs(lssum-ln*(lsum/ln)*(lsum/ln)) &lt; 0.001: leftAns = 0 else: leftAns = math.sqrt(lssum-ln*(lsum/ln)*(lsum/ln)) if math.fabs(rssum-rn*(rsum/rn)*(rsum/rn)) &lt; 0.001: rightAns = 0 else: rightAns = math.sqrt(rssum-rn*(rsum/rn)*(rsum/rn)) nowAns = leftAns + rightAns if(nowAns&lt;minVariance): minVariance = nowAns attributeNum = (curAttributeNum[i]+curAttributeNum[i+1])/2 i = i + 1 return minVariance,attributeNum 首先将每条数据的当前特质和当前特征值取出来，按照特征值的大小进行排序。 123456789101112131415class Pair: a = 0 b = 0 def __init__(self,a,b): self.a = a self.b = b def __lt__(self, other): return self.a &lt; other.aPairs = []for i in tree.items: Pairs.append(Pair(dataMat[i][curAttribute],dataMat[i][-1])) rn = rn + 1 rssum = rssum + dataMat[i][-1]*dataMat[i][-1] rsum = rsum + dataMat[i][-1]Pairs=sort(Pairs) 之后将数据集的特征值取出来，并按从小到大的顺序排好。 1curAttributeNum = sort(list(set(curAttributeNum))) 进行这两步操作是为了在遍历过程中，可以使复杂度达到线性，降低程序运行时间。之后进行具体候选特征值的均方差计算，求出最优结果。 123456789101112131415161718192021222324i = 0j = 0while i &lt; len(curAttributeNum)-1: while Pairs[j].a &lt;= (curAttributeNum[i]+curAttributeNum[i+1])/2: ln = ln + 1 lsum = lsum + Pairs[j].b lssum = lssum + Pairs[j].b*Pairs[j].b rn = rn - 1 rsum = rsum - Pairs[j].b rssum = rssum - Pairs[j].b*Pairs[j].b j = j + 1 if math.fabs(lssum-ln*(lsum/ln)*(lsum/ln)) &lt; 0.001: leftAns = 0 else: leftAns = math.sqrt(lssum-ln*(lsum/ln)*(lsum/ln)) if math.fabs(rssum-rn*(rsum/rn)*(rsum/rn)) &lt; 0.001: rightAns = 0 else: rightAns = math.sqrt(rssum-rn*(rsum/rn)*(rsum/rn)) nowAns = leftAns + rightAns if(nowAns&lt;minVariance): minVariance = nowAns attributeNum = (curAttributeNum[i]+curAttributeNum[i+1])/2 i = i + 1 5.剪枝剪枝的目的是为了防止过拟合，过拟合的意思就是当前模型对于训练集表现的很好，但是对验证数据集表现的很差，泛化能力很弱。这里的剪枝使用测试数据集进行剪枝，首先指定参数，使得构建出的树足够大、足够复杂，便于剪枝；接下来从上而下找到叶节点，用测试集来判断将这些叶节点合并是否能降低测试误差。如果是的话就合并。 12345基于已有的树切分测试数据： 如果存在任一子集是一棵树，则在该子集递归剪枝过程 计算将当前两个叶节点合并后的误差 计算不合并的误差 如果合并会降低误差的话，就将叶节点合并 calErrorNoMerge() 和 calErrorMerge() 用于计算两种处理方案的误差，以便于进行比较。 1234567891011121314151617def calErrorNoMerge(tree,lMat,rMat): error = 0 for item in lMat: val = item[-1] error = error + (val-tree.leftNode.num)*(val-tree.leftNode.num) for item in rMat: val = item[-1] error = error + (val-tree.rightNode.num)*(val-tree.rightNode.num) return errordef calErrorMerge(tree,testMat): error = 0 tree_num = (tree.leftNode.num+tree.rightNode.num)/2 for item in testMat: val = item[-1] error = error + (val-tree_num) * (val-tree_num) return error getMean() 是一个递归函数，它从上往下遍历树直到叶节点为止。如果找到两个叶节点则计算它们的平均值。该函数对树进行塌陷处理（即返回树平均值） 12345678910def getMean(tree): if tree.leftNode.isLeaf==0: getMean(tree.leftNode) if tree.rightNode.isLeaf==0: getMean(tree.rightNode) if tree.rightNode.isLeaf == 1 and tree.rightNode.isLeaf == 1: tree.num = (tree.rightNode.num+tree.leftNode.num)/2 tree.isLeaf = 1 tree.leftNode=tree.rightNode=None return splitSet() 用于划分测试集 1234567def splitSet(idx,num,testMat): lset = [] rset = [] for item in testMat: if(item[idx]&lt;=num): lset.append(item) else: rset.append(item) return lset,rset prune() 也是一个递归函数，接受的参数是当前节点和测试矩阵 12345678910111213141516171819def prune(tree,testMat): if(len(testMat)==0): getMean(tree) return lset,rset=splitSet(tree.label,tree.num,testMat) if tree.leftNode.isLeaf == 0: prune(tree.leftNode,lset) if tree.rightNode.isLeaf == 0: prune(tree.rightNode,rset) if tree.rightNode.isLeaf == 1 and tree.leftNode.isLeaf == 1: errorNoMerge = calErrorNoMerge(tree,lset,rset) errorMerge = calErrorMerge(tree,testMat) if(errorMerge&lt;errorNoMerge): tree.isLeaf = 1 tree.num = (tree.leftNode.num+tree.rightNode.num)/2 tree.leftNode = tree.rightNode = None print('merge') return return 首先判断测试集是否为空，如果为空则进行调用 getMean() 做塌陷处理。 123if(len(testMat)==0): getMean(tree) return 然后判断左分支和右分支是否存在，如果存在则进行递归。 1234if tree.leftNode.isLeaf == 0: prune(tree.leftNode,lset)if tree.rightNode.isLeaf == 0: prune(tree.rightNode,rset) 如果当前节点下有两个叶节点，则进行收否合并的判断。如果合并后的误差小于未合并的误差则进行合并，否则不进行合并。 123456789if tree.rightNode.isLeaf == 1 and tree.leftNode.isLeaf == 1: errorNoMerge = calErrorNoMerge(tree,lset,rset) errorMerge = calErrorMerge(tree,testMat) if(errorMerge&lt;errorNoMerge): tree.isLeaf = 1 tree.num = (tree.leftNode.num+tree.rightNode.num)/2 tree.leftNode = tree.rightNode = None print('merge') return 6.结果分析对于阀值的选择好坏是决定模型好坏的决定性因素，下表展示了在不同阀值下，模型对于同一测试数据集展现出的不同的误差。 阀值 剪枝前误差 剪枝后误差 7 31.454 29.234 6 14.911 10.351 5 13.283 8.722 4 13.665 7.467 3 11.887 7.327 2 11.417 6.789 1 8.379 5.699 0.5 4.485 2.259 0.1 4.565 2.342 从表中可以看出，当阀值在5及以下有着较小的误差，所以可以将阀值选择在5及以下。 7.可视化可视化用到是graphviz工具包 12345678910111213def printTree(tree,dot): if tree.isLeaf == 1: dot.node(name=str(tree),label='y=' + str(tree.num)) else: dot.node(name=str(tree), label=str(Label[tree.label]), color='green') if tree.faNode!=None: dot.edge(str(tree.faNode),str(tree),label='&lt;='+str(tree.faNode.num) if tree.isLeft == 1 else \"&gt;\"+str(tree.faNode.num)) if tree.isLeaf == 0: printTree(tree.leftNode,dot) printTree(tree.rightNode,dot)dot1 = Digraph(name=\"a\",comment=\"result1\",format=\"png\")printTree(root,dot1)dot1.view(filename=\"result\", directory=\"/Users/seven7777777/QQDownload/\") Digraph() 用于绘制一个图node() 用于绘制节点edge() 用于绘制边view() 用于保存图片并将图片显示出来 当取阀值为1时，生成的树： 点击此处查看 经过剪枝后：点击此处查看 ps:图片过大，加载需要一定时间 完整代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275from numpy import *import sysimport mathfrom graphviz import Digraphclass Node: faNode = None leftNode = None rightNode = None items = [] attributes = [] id = 0 label = 0 num = 0 isLeaf = 0 isLeft = 0 def __init__(self,items,attributes,label,num,isLeaf): self.items = items self.attributes = attributes self.label = label self.num = num self.isLeaf = isLeafdef loadFile(filengthame): dataMat = [] file = open(filengthame) for line in file.readlines(): curLine = line.strip().split('\\t') curLine = list(map(float,curLine)) dataMat.append(curLine) return dataMatdef loadFile1(filengthame): dataMat = [] file = open(filengthame) for line in file.readlines(): curLine = line.strip().split(' ') curLine = list(map(float,curLine)) dataMat.append(curLine) return dataMatdef init(): path = '/Users/seven7777777/QQDownload/airfoil_self_noise.dat' dataMat = loadFile(path) tree = Node([],[],'',0,0) i = 0 while i &lt; len(dataMat): tree.items.append(i) i = i + 1 i = 0 while i &lt; len(dataMat[0])-1: tree.attributes.append(i) i = i + 1 return tree,dataMatdef calVariance(curAttribute,curAttributeNum,tree,dataMat): attributeNum = 0 minVariance = sys.float_info.max lsum = 0;lssum = 0;ln = 0;rsum = 0;rssum = 0;rn = 0;dic = {} class Pair: a = 0 b = 0 def __init__(self,a,b): self.a = a self.b = b def __lt__(self, other): return self.a &lt; other.a Pairs = [] curAttributeNum = sort(list(set(curAttributeNum))) for i in tree.items: Pairs.append(Pair(dataMat[i][curAttribute],dataMat[i][-1])) rn = rn + 1 rssum = rssum + dataMat[i][-1]*dataMat[i][-1] rsum = rsum + dataMat[i][-1] Pairs=sort(Pairs) i = 0 j = 0 while i &lt; len(curAttributeNum)-1: while Pairs[j].a &lt;= (curAttributeNum[i]+curAttributeNum[i+1])/2: ln = ln + 1 lsum = lsum + Pairs[j].b lssum = lssum + Pairs[j].b*Pairs[j].b rn = rn - 1 rsum = rsum - Pairs[j].b rssum = rssum - Pairs[j].b*Pairs[j].b j = j + 1 if math.fabs(lssum-ln*(lsum/ln)*(lsum/ln)) &lt; 0.001: leftAns = 0 else: leftAns = math.sqrt(lssum-ln*(lsum/ln)*(lsum/ln)) if math.fabs(rssum-rn*(rsum/rn)*(rsum/rn)) &lt; 0.001: rightAns = 0 else: rightAns = math.sqrt(rssum-rn*(rsum/rn)*(rsum/rn)) nowAns = leftAns + rightAns if(nowAns&lt;minVariance): minVariance = nowAns attributeNum = (curAttributeNum[i]+curAttributeNum[i+1])/2 i = i + 1 return minVariance,attributeNumdef findBestFeatureToSplit(tree,dataMat): minVariance = sys.float_info.max attributeAns = 0 attributeNumAns = 0 for i in tree.attributes: curAttributeNum = [] for j in tree.items: curAttributeNum.append(dataMat[j][i]) j = j + 1 nowAns,attributeNum = calVariance(i,curAttributeNum,tree,dataMat) if(nowAns&lt;minVariance): minVariance = nowAns attributeAns = i attributeNumAns = attributeNum i = i + 1 return minVariance,attributeAns,attributeNumAnsdef calCurVariance(tree,dataMat): ssum = 0;sum=0 for i in tree.items: ssum = ssum + dataMat[i][-1]*dataMat[i][-1] sum = sum + dataMat[i][-1] return sqrt(ssum-len(tree.items)*(sum/len(tree.items))*(sum/len(tree.items))),sum/len(tree.items)def createTree(tree,dataMat,Top): curVariance,y = calCurVariance(tree,dataMat) if curVariance &lt; 1e-5: tree.num = y tree.isLeaf=1 return best,tree.label,tree.num = findBestFeatureToSplit(tree,dataMat) if math.fabs(best-curVariance) &lt; Top: tree.isLeaf=1 tree.num = 0 for i in tree.items: tree.num = tree.num + dataMat[i][-1] tree.num = tree.num/len(tree.items) return tree.leftNode = Node([],[],'',0,0) tree.rightNode = Node([],[],'',0,0) tree.leftNode.faNode = tree tree.rightNode.faNode = tree for i in tree.items: if (dataMat[i][tree.label] &lt;= tree.num): tree.leftNode.items.append(i) else: tree.rightNode.items.append(i) for i in tree.attributes: tree.leftNode.attributes.append(i) tree.rightNode.attributes.append(i) tree.leftNode.isLeft=1 createTree(tree.leftNode,dataMat,Top) createTree(tree.rightNode,dataMat,Top) returnLabel = [\"frequency\",\"angle\",\"xuanChang\",\"Free flow velocity\",\"Displacement thickness on suction side\"]n = 0def printTree(tree,dot): if tree.isLeaf == 1: dot.node(name=str(tree),label='y=' + str(tree.num)) else: dot.node(name=str(tree), label=str(Label[tree.label]), color='green') if tree.faNode!=None: dot.edge(str(tree.faNode),str(tree),label='&lt;='+str(tree.faNode.num) if tree.isLeft == 1 else \"&gt;\"+str(tree.faNode.num)) if tree.isLeaf == 0: printTree(tree.leftNode,dot) printTree(tree.rightNode,dot)def calErrorNoMerge(tree,lMat,rMat): error = 0 for item in lMat: val = item[-1] error = error + (val-tree.leftNode.num)*(val-tree.leftNode.num) for item in rMat: val = item[-1] error = error + (val-tree.rightNode.num)*(val-tree.rightNode.num) return errordef calErrorMerge(tree,testMat): error = 0 tree_num = (tree.leftNode.num+tree.rightNode.num)/2 for item in testMat: val = item[-1] error = error + (val-tree_num) * (val-tree_num) return errordef getMean(tree): if tree.leftNode.isLeaf==0: getMean(tree.leftNode) if tree.rightNode.isLeaf==0: getMean(tree.rightNode) if tree.rightNode.isLeaf == 1 and tree.rightNode.isLeaf == 1: tree.num = (tree.rightNode.num+tree.leftNode.num)/2 tree.isLeaf = 1 tree.leftNode=tree.rightNode=None returndef splitSet(idx,num,testMat): lset = [] rset = [] for item in testMat: if(item[idx]&lt;=num): lset.append(item) else: rset.append(item) return lset,rsetdef prune(tree,testMat): if(len(testMat)==0): getMean(tree) return lset,rset=splitSet(tree.label,tree.num,testMat) if tree.leftNode.isLeaf == 0: prune(tree.leftNode,lset) if tree.rightNode.isLeaf == 0: prune(tree.rightNode,rset) if tree.rightNode.isLeaf == 1 and tree.leftNode.isLeaf == 1: errorNoMerge = calErrorNoMerge(tree,lset,rset) errorMerge = calErrorMerge(tree,testMat) if(errorMerge&lt;errorNoMerge): tree.isLeaf = 1 tree.num = (tree.leftNode.num+tree.rightNode.num)/2 tree.leftNode = tree.rightNode = None print('merge') return returndef predict(tree,item): if(tree.isLeaf==1): return tree.num if item[tree.label]&lt;=tree.num:return predict(tree.leftNode,item) else: return predict(tree.rightNode,item)def fun(tree,testMat): #n = 1 sum = 0 L1 = [] L2 = [] for i in testMat: L1.append(predict(tree,i)) #print(\" 预测值:\",end=\"\"),print(predict(tree,i)) L2.append(i[-1]) #print(\" 实际值:\",end=\"\"),print(i[-1]) #n=n+1 i = 0 for i in range(0,len(L1)): sum = sum + (L1[i]-L2[i])*(L1[i]-L2[i]) sum = sum / len(L1) print(sum)testMat = loadFile1('/Users/seven7777777/QQDownload/test.dat')testMat2 = loadFile1('/Users/seven7777777/QQDownload/test2.dat')dot1 = Digraph(name=\"a\",comment=\"result1\",format=\"png\")dot2 = Digraph(name=\"b\",comment=\"result2\",format=\"png\")root,dataMat = init()createTree(root,dataMat,1)printTree(root,dot1)fun(root,testMat2)prune(root,testMat2)fun(root,testMat2)printTree(root,dot2)dot1.view(filename=\"result\", directory=\"/Users/seven7777777/QQDownload/\")dot2.view(filename=\"pruneResult\", directory=\"/Users/seven7777777/QQDownload/\")","link":"/2020/02/27/Cart%E6%A0%91%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"},{"title":"Bayesian Personalized Ranking from Implicit Feedback","text":"Bayesian Personalized Ranking from Implicit Feedback1 算法原理1.1 FeedbackExplicit Feedback 即显示反馈，用户可以明确的表示出对物品的喜好，比如用户对物品按照1-5星进行打分，不同的打分代表用户对物品不通的喜爱程度。 Implicit Feedback 即隐式反馈，用户不明确的表示出对物品的喜好，比如点赞或者收藏等等，这些都可以简单的成为选择（用户对物品的选择），除了用户选择的物品，还剩下未选择的，但是这不代表用户不喜欢这些物品，只是由于某种原因用户没有看到。 基于隐式反馈推荐算法有很多，比较常用的有矩阵分解或者k近邻等，但是这些算法并没有对排序的结果进行优化，而我们所关心的是哪些极少数物品在用户心目中有更高的优先级，BPR可以通过贝叶斯的角度来获取最大化后验估计。 1.2 BPR模型1.2.1 BPR的数据类型和性质在BPR算法中，对于一个用户 $u$ 在面对物品$i$和$j$时，若 选择 了物品$i$而没选择物品$j$，那么我们就可以说对于用户$u$来说，物品$i$在他心目中的重量是大于$j$的，因此我们可以得到一个三元组 $&lt;u,i,j&gt;$。一个训练集中三元组的数量是 $|U||I_u^+||I\\setminus I_u^+|$ ，其中 $U$ 为用户集，$I$ 为物品集，$I_u^+$ 为用户$u$选择的物品集。 在这里，我们有两个假设： 用户之间的偏好行为相互独立，即用户对物品之间的选择和其他用户无关。 同一用户对不同物品的选择相互独立，即用户当前物品$i,j$的选择和其他物无关。 在这里，我们使用 $&gt;_u$ 这个符号来表明用户的选择，即用户的偏好。则上面的三元组$&lt;u,i,j&gt;$可表示为$i&gt;_uj$。我们将所有的三元组表示为数据集 $D：={(u,i,j)|i\\in I_u^+ \\bigwedge j \\in I\\setminus I_u^+}$。 $&gt;_u$满足三个性质，对于用户集$U$和物品集$I$： 完整性：$\\forall i,j\\in I : i \\neq j \\Rightarrow i&gt;_uj \\cup j&gt;_ui$ 反对称性：$\\forall i,j\\in I :i&gt;_u j\\cap j&gt;_ui \\Rightarrow i=j$ 传递性：$\\forall i,j,k\\in I :i&gt;_uj\\cap j&gt;_uk \\Rightarrow i&gt;_uk:$ 1.2.2 矩阵分解对于用户集$U$和物品集$I$对应的排序矩阵$\\overline{X}(|U| \\times |I|)$如下图所示 为了能够使用量化该矩阵，即量化矩阵中的$+ ?$我们必须对该矩阵进行分解，分解出一个用户矩阵$W(|U| \\times k)$和物品矩阵$H(|I|\\times k)$,使得$$\\overline{X}=WH^T$$这里的$k$是自己定义的，一般远小于 $|U|$ 和 $|I|$。 对于任何一个用户 $u$ ，其对应的任何一个物品$i$，排序分数计算如下$$\\overline{x}{ui}=w_u\\bullet h_i=\\sum{l=1}^{k}w_{ul}h_{il}$$因此，我们训练的目标就是找到合适的$W$和$H$,使得 $\\overline{X}$ 和 $X$ 最相似。这一点和Factorization Machine中使用$V$求解 $\\overline{W}$ 的效果是一样的。 1.3 使用贝叶斯对模型进行优化这里我们使用$\\Theta$来代表上文所提到的参数$W$和$H$，$&gt;_u$代表用户$u$对所以商品的全序关系，则优化的目标是：$$P(\\Theta|&gt;_u)$$根据贝叶斯公式有：$$P(\\Theta|&gt;_u)=\\frac{P(&gt;_u|\\Theta)P(\\Theta)}{P(&gt;_u)}$$根据上文的第一个假设，用户之间的偏好行为相互独立，所以对于任何一个用户$u$，$P(&gt;_u)$对所有的物品是一样的，因此有：$$P(\\Theta|&gt;_u)\\propto P(&gt;_u|\\Theta)P(\\Theta)$$这里的 $\\propto$ 表示正比于，上面的正比关系是显而易见的。 这样，优化的目标就转化为了两部分似然函数 $P(&gt;_u|\\Theta)$ 和先验概率$P(\\Theta)$，前者和数据集$D$有关，后者和数据集$D$无关。 1.3.1 似然函数的优化由于我们假设了用户间相互独立，同一用户对不同物品的选择也独立，因此有：$$\\prod_{u \\in U}P(&gt;u|\\Theta) = \\prod{(u,i,j) \\in (U \\times I \\times I)}P(i &gt;u j|\\Theta)^{\\delta((u,i,j) \\in D)}(1-P(i &gt;_u j|\\Theta))^{\\delta((u,j,i) \\not\\in D) }$$这里关于正例的表达 $P(i &gt;_u j|\\Theta)^{\\delta((u,i,j) \\in D)}$ 是好理解的，但是关键在于负例的表达 $(1-P(i &gt;_u j|\\Theta))^{\\delta((u,j,i) \\not\\in D) }$ ，上文已经提到 $D：={(u,i,j)|i\\in I_u^+ \\bigwedge j \\in I\\setminus I_u^+}$ ，这就说明这里的$i$和$j$是有限制的，假设当前$I={1,2,3,4}$，对于一个用户$u$，$I_u^+={1,3}$，则$I\\setminus I_u^+={2,4}$，此时的正例就为$$D+={(u,i_1,j_2),(u,i_1,j_4),(u,i_3,j_2),(u,i_3,j_4)}$$所对应的反例应为$$D_-={(u,j_1,i_2),(u,j_1,i_4),(u,j_3,i_2),(u,j_3,i_4)}$$ 而且这里的$(u,i,j)$是指训练集的三元组，而不是所有的三元组。所以这种表达方式是可以的。 其中：$$\\delta(b)= \\begin{cases} 1&amp; {if; b; is ;true}\\ 0&amp; {else} \\end{cases}$$根据完整性和反对称性，正例和反例同效，因此有：$$\\prod_{u \\in U}P(&gt;u|\\Theta) = \\prod{(u,i,j) \\in (U \\times I \\times I)}P(i &gt;u j|\\Theta)^{\\delta((u,i,j) \\in D)}$$只有$(u,i,j)\\in D$时，才有$\\delta((u,i,j) \\in D)=1$，因此有$$\\prod{(u,i,j) \\in (U \\times I \\times I)}P(i &gt;u j|\\Theta)^{\\delta((u,i,j) \\in D)} =\\prod{(u,i,j) \\in D}P(i &gt;_u j|\\Theta)$$ 因此$$\\prod_{u \\in U}P(&gt;u|\\Theta) = \\prod{(u,i,j) \\in D}P(i &gt;u j|\\Theta)$$对于$ P(i &gt;_u j|\\Theta)$，我们可以使用以下式子代替：$$P(i &gt;_u j|\\Theta) = \\sigma(\\overline{x}{uij}(\\Theta))$$其中：$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$即 $sigmoid$ 函数，这个函数满足了上面所提到了三个性质。 对于 $\\overline{x}{uij}(\\Theta)$ 我们可以发现，当满足 $i&gt;_uj$ 时， $\\overline{x}{uij}(\\Theta)&gt;0$ ，否则当 $j&gt;ui$ 时，$\\overline{x}{uij}(\\Theta)&lt;0$，我们因此可以将这个式子转变为与其等价的下面的公式：$$\\overline{x}{uij}(\\Theta) = \\overline{x}{ui}(\\Theta) - \\overline{x}_{uj}(\\Theta)$$其他等价的式子也是可以的。 最终，$P(&gt;u|\\Theta)$的优化结果为：$$\\prod{u \\in U}P(&gt;u|\\Theta) = \\prod{(u,i,j) \\in D} \\sigma(\\overline{x}{ui} - \\overline{x}{uj})$$ 1.3.2 先验概率的优化对于这一部分，原文作者假设这一部分的概率服从正态分布，对应的均值为0，协方差矩阵为$\\Sigma_{\\Theta}=\\lambda_{\\Theta}I$，即：$$P(\\Theta) \\sim N(0, \\Sigma_{\\Theta})$$则对于$lnP(\\Theta)$有：$$lnP(\\Theta)=ln(\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma_ \\Theta|^{\\frac{1}{2}}}exp(-\\frac{1}{2}\\frac{\\Theta^T\\Theta}{\\Sigma_\\Theta}))\\=-\\frac{1}{2}\\frac{\\Theta^T\\Theta}{\\Sigma_\\Theta}-ln(\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma_ \\Theta|^{\\frac{1}{2}}})$$因此：$$lnP(\\Theta) \\propto -\\lambda_{\\Theta}||\\Theta||^2$$所以这里$lnP(\\Theta)可以表示成一个L2正则项$，这里的$\\lambda_\\Theta$是一个超参数，因此正负都可以，这里为了符合直观逻辑，我用了负号。 1.3.3 优化结果由1.3.1和1.3.2可得最后的优化结果为：$$ln;P(\\Theta|&gt;u) \\propto ln;P(&gt;_u|\\Theta)P(\\Theta) \\= ln;\\prod\\limits{(u,i,j) \\in D} \\sigma(\\overline{x}{ui} - \\overline{x}{uj}) + ln P(\\Theta)\\ = \\sum\\limits_{(u,i,j) \\in D}ln\\sigma(\\overline{x}{ui} - \\overline{x}{uj}) - \\lambda_\\Theta||\\Theta||^2;$$ 1.3.4参数求导由于$$(ln\\sigma(\\Theta))’=1-\\sigma(\\Theta)=1-\\frac{1}{1+e^{-\\Theta}}=\\frac{1}{1+e^\\Theta}$$则优化目标对参数$\\Theta$的求导为：$$\\frac{\\partial ln;P(\\Theta|&gt;u)}{\\partial \\Theta} \\propto \\sum\\limits{(u,i,j) \\in D} \\frac{1}{1+e^{\\overline{x}{ui} - \\overline{x}{uj}}}\\frac{\\partial (\\overline{x}{ui} - \\overline{x}{uj})}{\\partial \\Theta} - \\lambda \\Theta$$由于$$\\overline{x}{ui} - \\overline{x}{uj} = \\sum\\limits_{l=1}^kw_{ul}h_{il} - \\sum\\limits_{l=1}^kw_{ul}h_{jl}$$所以$$\\frac{\\partial (\\overline{x}{ui} - \\overline{x}{uj})}{\\partial \\Theta} = \\begin{cases} (h_{il}-h_{jl})&amp; {if; \\Theta = w_{ul}}\\ w_{ul}&amp; {if;\\Theta = h_{il}} \\ -w_{ul}&amp; {if;\\Theta = h_{jl}}\\end{cases}$$ 1.3.5 BPR算法流程经过1.3.4的推导，已经得到了各个参数的求导结果，为了使得$ln;P(\\Theta|&gt;_u)$取得较大的值，使用梯度上升来进行训练，算法流程如下： 输入：训练集$D$三元组，梯度步长$\\alpha$， 正则化参数$\\lambda$,分解矩阵维度$k$ 输出：$\\Theta(W,H)$ 1: 初始化$\\Theta$ 2: 迭代 3: 对于每一个$(u,i,j)\\in D$ 4: $\\Theta \\leftarrow \\Theta + \\alpha(\\frac{1}{1+e^{\\overline{x}{ui} - \\overline{x}{uj}}}\\frac{\\partial (\\overline{x}{ui} - \\overline{x}{uj})}{\\partial \\Theta} - \\lambda \\Theta)$ 5: 直至$\\Theta$收敛 6: 返回$\\Theta$ 7：计算每个用户$u$对每个商品的排序分:$\\overline{x}_{ui} = w_u \\bullet h_i$，选择排序分高的诺干个商品输出 8：结束 1.3.6 AUC的计算对于每一个用户$u$，他的$AUC$可表示为:$$AUC(u):=\\frac{1}{|I_u^+||I\\setminus I_u^+|}\\sum_{i\\in I_u^+}\\sum_{j\\in I\\setminus I_u^+}\\delta(\\overline{x}{uij}&gt;0)$$总的$AUC$可表示为：$$AUC:=\\frac{1}{|U|}\\sum{u\\in U} AUC(u)$$ 2 BPR的简单实现2.1数据预处理这里的数据集使用的是ml-latest-small(1MB)里的ratings.csv，数据格式如下 userID（用户号） moiveID（电影号） rating（评分） Timestamap（评分时间） 1 1 4 964982703 1 3 4 964981247 1 6 4 964982224 1 47 5 964983815 … … … … 这里我们把用户对电影的评分作为选择，因此我们只使用前两列的数据（userID，moiveID）就可以了。 读取数据集 统计用户数目、电影数目、和每个用户所评分的电影 12345678910111213141516171819def load_data(data_path): sum_users = 0 sum_items = 0 user_ratings = {} data = [] with open(data_path, 'r') as f: reader = csv.reader(f) for row in islice(reader, 1, None): row[0] = int(row[0]) row[1] = int(row[1]) if row[0] not in user_ratings: user_ratings[row[0]] = set() user_ratings[row[0]].add(row[1]) sum_users = max(row[0], sum_users) sum_items = max(row[1], sum_items) return sum_users, sum_items, user_ratings 预构造测试数据 为每个用户在他评分的电影中随机选择一个电影 选择之后可将该电影在该用户评分的电影集合中移除 12345678def generate_test(user_ratings): user_rating_test = {} for user, moives in user_ratings.items(): user_rating_test[user] = random.sample(user_ratings[user], 1)[0] user_ratings[user].remove(user_rating_test[user]) # print(user_rating_test) return user_rating_test 构造训练三元组 因为只是简单的描述算法逻辑，所以随机选择一个三元组。 选择的三元组不在测试集合里。 123456789def generate_train_set(user_ratings, user_rating_test, sum_movies): user = random.sample(user_ratings.keys(), 1)[0] watch = random.sample(user_ratings[user], 1)[0] not_watch = random.randint(1, sum_movies) while not_watch in user_ratings[user] or not_watch == user_rating_test[user]: not_watch = random.randint(1, sum_movies) train_set= [user - 1, watch - 1, not_watch - 1] return np.asarray(train_set) 构造测试集合 在之前的预构造测试集操作完成后，对于每个用户我们已经挑选出了一个选择的电影，之后遍历其未选择的电影，构造测试集合。 因为只是简单的描述算法逻辑，设置cnt的作用是选取一部分测试集合。 1234567891011def generate_test_set(user_ratings,user_rating_test,sum_movies): test_set = [] for user in user_ratings.keys(): cnt = 0 for movie in range(sum_movies): if movie not in user_ratings[user] and not movie == user_rating_test[user]: test_set.append([user-1,user_rating_test[user]-1,movie]) cnt += 1 if cnt &gt; 50 :break return np.asarray(test_set) 2.2 创建BPR类 详情见注释 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118class BPR(): def __init__(self, L2_rate, learn_rate, epoch, batch_size, sum_users, sum_movies, hidden_factor): self.L2_rate = L2_rate # L2正则化系数 self.learn_rate = learn_rate # 训练学习率 self.epoch = epoch # 训练周期次数 self.batch_size = batch_size # 选取的训练三元组数量 self.sum_users = sum_users # 用户数量 self.sum_movies = sum_movies # 电影数量 self.hidden_factor = hidden_factor # 矩阵分解中的第二维度大小 self.initialize_weights() #初始化W，H def train_model(self,u,i,j): # 对模型进行训练 # 获取 ui uj ui = self.w[u]*self.h[i] uj = self.w[u]*self.h[j] uij = np.sum(ui-uj) # l2正则 l2_loss = np.sum(self.w[u] * self.w[u] + \\ self.h[i] * self.h[i] + \\ self.h[j] * self.h[j]) #损失 loss = self.L2_rate*l2_loss - np.log(sigmoid(uij)) common = 1.0/(1.0+np.exp(uij)) #梯度上升 self.w[u] += self.learn_rate * (common*(self.h[i]-self.h[j])-self.L2_rate*self.w[u]) self.h[i] += self.learn_rate * (common*self.w[u]-self.L2_rate*self.h[i]) self.h[i] += self.learn_rate * (common*(-self.w[u])-self.L2_rate * self.h[i]) return loss def initialize_weights(self): #初始化W，H self.w = np.random.normal(0,0.1,size=[self.sum_users,self.hidden_factor]) self.h = np.random.normal(0,0.1,size=[self.sum_movies,self.hidden_factor]) # 选择训练三元祖进行训练 def train(self, user_ratings, user_rating_test): print(\"training......\\n\") for epoch in range(self.epoch): all_loss = 0 for _batch_size in range(self.batch_size): #随机选择一个训练三元组 train_set = generate_train_set(user_ratings, user_rating_test, self.sum_movies) #获取单次训练损失 bpr_loss = self.train_model(train_set[0],train_set[1],train_set[2]) all_loss += bpr_loss mean_loss = all_loss / self.batch_size print('----------------------------------------------------') print('epoch : ', epoch+1) print('loss : ', mean_loss) print('----------------------------------------------------\\n') print(\"end train\\n\") #使用测试集进行验证 def predict(self,test_set,user_ratings,sum_movies): print(\"predicting......\\n\") all_loss = 0 bpr_auc = 0 user_auc = {} for _uij in test_set: u,i,j=_uij[0],_uij[1],_uij[2] uij = self.w[u]*self.h[i]-self.w[u]*self.h[j] #获取每个用户的预测结果 if u not in user_auc: user_auc[u] = [] user_auc[u].append(uij) #单次损失 bpr_loss = self.L2_rate*np.sum(self.w[u] * self.w[u] + self.h[i] * self.h[i] + self.h[j] * self.h[j])\\ - np.log(sigmoid(np.sum(uij))) #总损失 all_loss += bpr_loss #计算每个用户的auc和总的auc for user in user_auc.keys(): user_auc[user] = np.asarray(user_auc[user]) Counter(user_auc[user].flatten()) bpr_auc += sum(user_auc[user].flatten() &gt; 0) * 1.0 / (len(user_auc[user].flatten())) print('----------------------------------------------------') print('predict_loss :',all_loss/len(test_set)) print('predict_auc :', 1.0*bpr_auc / len(user_auc)) print('----------------------------------------------------\\n') print(\"end predict\\n\") #为每个用户推荐前first个电影 def get_rank_by_user(self,user,first): print(\"ranking......\\n\",\"user : \",user,\"\\n\") user_to_movie = np.matmul(self.w[user],np.transpose(self.h)) arg_sort = np.argsort(-user_to_movie) rank = 1 for movie in arg_sort[:first]: print('----------------------------------------------------') print('rank',rank) rank += 1 print('movie : ',movie) print(\"score : \",user_to_movie[movie]) print('----------------------------------------------------\\n') print(\"end rank\\n\") 2.3 完整代码 运行方法：加载项目至pychram或其他IDE中即可运行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200import randomimport numpy as npimport csvfrom itertools import islicefrom collections import Counterdef sigmoid(x): s = 1.0 / (1.0 + np.exp(-x)) return sdef load_data(data_path): sum_users = 0 sum_movies = 0 user_ratings = {} with open(data_path, 'r') as f: reader = csv.reader(f) for row in islice(reader, 1, None): row[0] = int(row[0]) row[1] = int(row[1]) if row[0] not in user_ratings: user_ratings[row[0]] = set() user_ratings[row[0]].add(row[1]) sum_users = max(row[0], sum_users) sum_movies = max(row[1], sum_movies) # print('sum_users : ',sum_users) # print('sum_movies : ',sum_movies) return sum_users, sum_movies, user_ratingsdef generate_test(user_ratings): user_rating_test = {} for user, moives in user_ratings.items(): user_rating_test[user] = random.sample(user_ratings[user], 1)[0] user_ratings[user].remove(user_rating_test[user]) # print(user_rating_test) return user_rating_testdef generate_train_set(user_ratings, user_rating_test, sum_movies): user = random.sample(user_ratings.keys(), 1)[0] watch = random.sample(user_ratings[user], 1)[0] not_watch = random.randint(1, sum_movies) while not_watch in user_ratings[user] or not_watch == user_rating_test[user]: not_watch = random.randint(1, sum_movies) train_set= [user - 1, watch - 1, not_watch - 1] return np.asarray(train_set)def generate_test_set(user_ratings,user_rating_test,sum_movies): test_set = [] for user in user_ratings.keys(): cnt = 0 for movie in range(sum_movies): if movie not in user_ratings[user] and not movie == user_rating_test[user]: test_set.append([user-1,user_rating_test[user]-1,movie]) cnt += 1 if cnt &gt; 50 :break return np.asarray(test_set)class BPR(): def __init__(self, L2_rate, learn_rate, epoch, batch_size, sum_users, sum_movies, hidden_factor): self.L2_rate = L2_rate # L2正则化系数 self.learn_rate = learn_rate # 训练学习率 self.epoch = epoch # 训练周期次数 self.batch_size = batch_size # 选取的训练三元组数量 self.sum_users = sum_users # 用户数量 self.sum_movies = sum_movies # 电影数量 self.hidden_factor = hidden_factor # 矩阵分解中的第二维度大小 self.initialize_weights() # 初始化W，H def train_model(self, u, i, j): # 对模型进行训练 # 获取 ui uj ui = self.w[u] * self.h[i] uj = self.w[u] * self.h[j] uij = np.sum(ui - uj) # l2正则 l2_loss = np.sum(self.w[u] * self.w[u] + \\ self.h[i] * self.h[i] + \\ self.h[j] * self.h[j]) # 损失 loss = self.L2_rate * l2_loss - np.log(sigmoid(uij)) common = 1.0 / (1.0 + np.exp(uij)) # 梯度上升 self.w[u] += self.learn_rate * (common * (self.h[i] - self.h[j]) - self.L2_rate * self.w[u]) self.h[i] += self.learn_rate * (common * self.w[u] - self.L2_rate * self.h[i]) self.h[i] += self.learn_rate * (common * (-self.w[u]) - self.L2_rate * self.h[i]) return loss def initialize_weights(self): # 初始化W，H self.w = np.random.normal(0, 0.1, size=[self.sum_users, self.hidden_factor]) self.h = np.random.normal(0, 0.1, size=[self.sum_movies, self.hidden_factor]) # 选择训练三元祖进行训练 def train(self, user_ratings, user_rating_test): print(\"training......\\n\") for epoch in range(self.epoch): all_loss = 0 for _batch_size in range(self.batch_size): # 随机选择一个训练三元组 train_set = generate_train_set(user_ratings, user_rating_test, self.sum_movies) # 获取单次训练损失 bpr_loss = self.train_model(train_set[0], train_set[1], train_set[2]) all_loss += bpr_loss mean_loss = all_loss / self.batch_size print('----------------------------------------------------') print('epoch : ', epoch + 1) print('loss : ', mean_loss) print('----------------------------------------------------\\n') print(\"end train\\n\") # 使用测试集进行验证 def predict(self, test_set, user_ratings, sum_movies): print(\"predicting......\\n\") all_loss = 0 bpr_auc = 0 user_auc = {} for _uij in test_set: u, i, j = _uij[0], _uij[1], _uij[2] uij = self.w[u] * self.h[i] - self.w[u] * self.h[j] # 获取每个用户的预测结果 if u not in user_auc: user_auc[u] = [] user_auc[u].append(uij) # 单次损失 bpr_loss = self.L2_rate * np.sum(self.w[u] * self.w[u] + self.h[i] * self.h[i] + self.h[j] * self.h[j]) \\ - np.log(sigmoid(np.sum(uij))) # 总损失 all_loss += bpr_loss # 计算每个用户的auc和总的auc for user in user_auc.keys(): user_auc[user] = np.asarray(user_auc[user]) Counter(user_auc[user].flatten()) bpr_auc += sum(user_auc[user].flatten() &gt; 0) * 1.0 / (len(user_auc[user].flatten())) print('----------------------------------------------------') print('predict_loss :', all_loss / len(test_set)) print('predict_auc :', 1.0 * bpr_auc / len(user_auc)) print('----------------------------------------------------\\n') print(\"end predict\\n\") # 为每个用户推荐前first个电影 def get_rank_by_user(self, user, first): print(\"ranking......\\n\", \"user : \", user, \"\\n\") user_to_movie = np.matmul(self.w[user], np.transpose(self.h)) arg_sort = np.argsort(-user_to_movie) rank = 1 for movie in arg_sort[:first]: print('----------------------------------------------------') print('rank', rank) rank += 1 print('movie : ', movie) print(\"score : \", user_to_movie[movie]) print('----------------------------------------------------\\n') print(\"end rank\\n\")def bpr_function(): data_path = '../data/ratings.csv' sum_users, sum_movies, user_ratings = load_data(data_path) print('sum_users: ',sum_users,\"sum_movies: \",sum_movies,'\\n') user_rating_test = generate_test(user_ratings) model = BPR(0.001, 0.01, 10, 4096*2, sum_users, sum_movies, 3) model.train(user_ratings, user_rating_test) test_set = generate_test_set(user_ratings,user_rating_test,sum_movies) model.predict(test_set,user_ratings,sum_movies) model.get_rank_by_user(1,5)if __name__ == '__main__': bpr_function() 2.4 运行结果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899sum_users: 610 sum_movies: 193609training......----------------------------------------------------epoch : 1loss : 0.6934863013894647--------------------------------------------------------------------------------------------------------epoch : 2loss : 0.6932062795722178--------------------------------------------------------------------------------------------------------epoch : 3loss : 0.6931844325030657--------------------------------------------------------------------------------------------------------epoch : 4loss : 0.6931677007031275--------------------------------------------------------------------------------------------------------epoch : 5loss : 0.6931695717243582--------------------------------------------------------------------------------------------------------epoch : 6loss : 0.6932285823146795--------------------------------------------------------------------------------------------------------epoch : 7loss : 0.6932990431675607--------------------------------------------------------------------------------------------------------epoch : 8loss : 0.6931254845390923--------------------------------------------------------------------------------------------------------epoch : 9loss : 0.6932871538235206--------------------------------------------------------------------------------------------------------epoch : 10loss : 0.693114981810056----------------------------------------------------end trainpredicting......----------------------------------------------------predict_loss : 0.6930070847907785predict_auc : 0.5071359691417545----------------------------------------------------end predictranking...... user : 1----------------------------------------------------rank 1movie : 188708score : 0.040865749655743536--------------------------------------------------------------------------------------------------------rank 2movie : 95869score : 0.039913043489193896--------------------------------------------------------------------------------------------------------rank 3movie : 18868score : 0.03873554582186161--------------------------------------------------------------------------------------------------------rank 4movie : 120526score : 0.03847357646550457--------------------------------------------------------------------------------------------------------rank 5movie : 13526score : 0.03829975463477729----------------------------------------------------end rank 3 总结 BPR本身不是一个推荐算法，只是通过贝叶斯后验估计来优化排序的过程，推荐算法本身是通过矩阵分解完成的。 在推导的过程中用到了很多的比例关系，这在概率问题上面有着显著的作用。 对于论文中的一些推导，论文中并没有明确的说到具体过程，需要自己去理解。 本文只是对论文中的过程进行了大致的描述，忽略掉了论文中的一些细节。 贝叶斯定理的功能在本算法中体现了重要的作用，我相信该定理还有许多可以应用的场景，需要我们这些“后浪”一起去发掘。 在实现过程中，仅使用了一小部分数据进行程序的运行，所以运行结果可能不太准确。 由于时间有限和个人水平有限，本文难免有些纰漏，恳请您指出！","link":"/2020/08/30/Bayesian%20Personalized%20Ranking%20from%20Implicit%20Feedback/"},{"title":"educoder数据挖掘算法原理与实践：ID3决策树","text":"引例在炎热的夏天，没有什么比冰镇后的西瓜更能令人感到心旷神怡的了。现在我要去水果店买西瓜，但什么样的西瓜能入我法眼呢？那根据我的个人习惯，在挑西瓜时可能就有这样的脑回路。 假设现在水果店里有3个西瓜，它们的属性如下：那么根据我的脑回路我会买1和2号西瓜。其实我的脑回路可以看成一棵树，并且这颗树能够帮助我对买不买西瓜这件事做决策，所以它就是一棵决策树。 决策树的相关概念决策树是一种可以用于分类与回归的机器学习算法，但主要用于分类。用于分类的决策树是一种描述对实例进行分类的树形结构。决策树由结点和边组成，其中结点分为内部结点和叶子结点，内部结点表示一个特征或者属性，叶子结点表示标签（脑回路图中黄色的是内部结点，蓝色的是叶子结点）。 从代码角度来看，决策树其实可以看成是一堆if-else语句的集合，例如引例中的决策树完全可以看成是如下代码： 12345678910111213if isRed: if isCold: if hasSeed: print(\"buy\") else: print(\"don't buy\") else: if isCheap: print(\"buy\") else: print(\"don't buy\")else: print(\"don't buy\") 因此决策树的一个非常大的优势就是模型的可理解性非常高，甚至可以用来挖掘数据中比较重要的信息。 那么如何构造出一棵好的决策树呢？其实构造决策树时会遵循一个指标，有的是按照信息增益来构建，如ID3算法；有的是信息增益率来构建，如C4.5算法；有的是按照基尼系数来构建的，如CART算法。但不管是使用哪种构建算法，决策树的构建过程通常都是一个递归选择最优特征，并根据特征对训练集进行分割，使得对各个子数据集有一个最好的分类的过程。 这一过程对应着对特征空间的划分，也对应着决策树的构建。一开始，构建决策树的根结点，将所有训练数据都放在根结点。选择一个最优特征，并按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶子结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，并构建相应的结点。如此递归进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶子结点上，即都有了明确的类别。这就构建出了一棵决策树。 信息熵信息是个很抽象的概念。人们常常说信息很多，或者信息较少，但却很难说清楚信息到底有多少。比如一本五十万字的中文书到底有多少信息量。 直到1948年，香农提出了“信息熵”的概念，才解决了对信息的量化度量问题。信息熵这个词是香农从热力学中借用过来的。热力学中的热熵是表示分子状态混乱程度的物理量。香农用信息熵的概念来描述信源的不确定度。信源的不确定性越大，信息熵也越大。 从机器学习的角度来看，信息熵表示的是信息量的期望值。如果数据集中的数据需要被分成多个类别，则信息量 $I(x_i)$ 的定义如下(其中$x_i$表示多个类别中的第i个类，$p(x_i)$数据集中类别为$x_i$的数据在数据集中出现的概率表示)：$$I(X_i)=-log_2p(x_i)$$由于信息熵是信息量的期望值，所以信息熵H(X)的定义如下(其中n为数据集中类别的数量)：$$H(X)=-\\sum_{i=1}^{n}p(x_i)log_2p(x_i)$$ 从这个公式也可以看出，如果概率是0或者是1的时候，熵就是0。（因为这种情况下随机变量的不确定性是最低的），那如果概率是0.5也就是五五开的时候，此时熵达到最大，也就是1。（就像扔硬币，你永远都猜不透你下次扔到的是正面还是反面，所以它的不确定性非常高）。所以呢，熵越大，不确定性就越高。 条件熵在实际的场景中，我们可能需要研究数据集中某个特征等于某个值时的信息熵等于多少，这个时候就需要用到条件熵。条件熵H(Y|X)表示特征X为某个值的条件下，类别为Y的熵。条件熵的计算公式如下： $$H(Y|X)=\\sum^{n}_{i=1}p_iH(Y|X=x_i)$$ 当然条件熵的一个性质也熵的性质一样，概率越确定，条件熵就越小，概率越五五开，条件熵就越大。 信息增益现在已经知道了什么是熵，什么是条件熵。接下来就可以看看什么是信息增益了。所谓的信息增益就是表示我已知条件X后能得到信息Y的不确定性的减少程度。 就好比，我在玩读心术。你心里想一件东西，我来猜。我已开始什么都没问你，我要猜的话，肯定是瞎猜。这个时候我的熵就非常高。然后我接下来我会去试着问你是非题，当我问了是非题之后，我就能减小猜测你心中想到的东西的范围，这样其实就是减小了我的熵。那么我熵的减小程度就是我的信息增益。 所以信息增益如果套上机器学习的话就是，如果把特征A对训练集D的信息增益记为g(D, A)的话，那么g(D, A)的计算公式就是： $$g(D, A)=H(D)-H(D,A)g(D,A)=H(D)−H(D,A)$$ 为了更好的解释熵，条件熵，信息增益的计算过程，下面通过示例来描述。假设我现在有这一个数据集，第一列是编号，第二列是性别，第三列是活跃度，第四列是客户是否流失的标签（0:表示未流失，1:表示流失）。假如要算性别和活跃度这两个特征的信息增益的话，首先要先算总的熵和条件熵。总的熵其实非常好算，就是把标签作为随机变量X。上表中标签只有两种（0和1）因此随机变量X的取值只有0或者1。所以要计算熵就需要先分别计算标签为0的概率和标签为1的概率。从表中能看出标签为0的数据有10条，所以标签为0的概率等于2/3。标签为1的概率为1/3。所以熵为： $$-(1/3)*log(1/3)-(2/3)*log(2/3)=0.9182−(1/3)∗log(1/3)−(2/3)∗log(2/3)=0.9182$$ 接下来就是条件熵的计算，以性别为男的熵为例。表格中性别为男的数据有8条，这8条数据中有3条数据的标签为1，有5条数据的标签为0。所以根据条件熵的计算公式能够得出该条件熵为： $$-(3/8)*log(3/8)-(5/8)*log(5/8)=0.9543−(3/8)∗log(3/8)−(5/8)∗log(5/8)=0.9543$$ 根据上述的计算方法可知，总熵为： $$-(5/15)*log(5/15)-(10/15)*log(10/15)=0.9182−(5/15)∗log(5/15)−(10/15)∗log(10/15)=0.9182$$ 性别为男的熵为： $$-(3/8)*log(3/8)-(5/8)*log(5/8)=0.9543−(3/8)∗log(3/8)−(5/8)∗log(5/8)=0.9543$$ 性别为女的熵为： $$-(2/7)*log(2/7)-(5/7)*log(5/7)=0.8631−(2/7)∗log(2/7)−(5/7)∗log(5/7)=0.8631$$ 活跃度为低的熵为： $$-(4/4)*log(4/4)-0=0−(4/4)\\∗log(4/4)−0=0$$ 活跃度为中的熵为： $$-(1/5)*log(1/5)-(4/5)*log(4/5)=0.7219−(1/5)∗log(1/5)−(4/5)∗log(4/5)=0.7219$$ 活跃度为高的熵为： $$-0-(6/6)*log(6/6)=0−0−(6/6)∗log(6/6)=0$$ 现在有了总的熵和条件熵之后就能算出性别和活跃度这两个特征的信息增益了。 性别的信息增益=总的熵-(8/15)性别为男的熵-(7/15)性别为女的熵=0.0064 活跃度的信息增益=总的熵-(6/15)活跃度为高的熵-(5/15)活跃度为中的熵-(4/15)*活跃度为低的熵=0.6776 那信息增益算出来之后有什么意义呢？回到读心术的问题，为了我能更加准确的猜出你心中所想，我肯定是问的问题越好就能猜得越准！换句话来说我肯定是要想出一个信息增益最大（减少不确定性程度最高）的问题来问你。其实ID3算法也是这么想的。ID3算法的思想是从训练集D中计算每个特征的信息增益，然后看哪个最大就选哪个作为当前结点。然后继续重复刚刚的步骤来构建决策树。 ID3算法就是从根结点开始，对结点计算所有可能的特征的信息增益，然后选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点，然后对子结点递归执行上述的步骤直到信息增益很小或者没有特征可以继续选择为止。 因此，ID3算法伪代码如下： 1234567891011121314151617#假设数据集为D，标签集为A，需要构造的决策树为treedef ID3(D, A): if D中所有的标签都相同: return 标签 if 样本中只有一个特征或者所有样本的特征都一样: 对D中所有的标签进行计数 return 计数最高的标签 计算所有特征的信息增益 选出增益最大的特征作为最佳特征(best_feature) 将best_feature作为tree的根结点 得到best_feature在数据集中所有出现过的值的集合(value_set) for value in value_set: 从D中筛选出best_feature=value的子数据集(sub_feature) 从A中筛选出best_feature=value的子标签集(sub_label) #递归构造tree tree[best_feature][value] = ID3(sub_feature, sub_label) return tree 使用决策树进行预测决策树的预测思想非常简单，假设现在已经构建出了一棵用来决策是否买西瓜的决策树。 并假设现在在水果店里有这样一个西瓜，其属性如下：那买不买这个西瓜呢？只需把西瓜的属性代入决策树即可。决策树的根结点是瓤是否够红，所以就看西瓜的属性，经查看发现够红，因此接下来就看够不够冰。而西瓜不够冰，那么看是否便宜。发现西瓜是便宜的，所以这个西瓜是可以买的。 因此使用决策树进行预测的伪代码也比较简单，伪代码如下： 12345678#tree表示决策树，feature表示测试数据def predict(tree, feature): if tree是叶子结点: return tree 根据feature中的特征值走入tree中对应的分支 if 分支依然是课树: result = predict(分支, feature) return result 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197#encoding=utf8import numpy as npfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_split# 计算熵def calcInfoEntropy(label): ''' input: label(narray):样本标签 output: InfoEntropy(float):熵 ''' #********* Begin *********# # 计算标签在数据集中出现的概率 dic = {} for i in label: if i not in dic: dic[i] = 1 else: dic[i] += 1 # 计算熵 num = len(label) InfoEntropy = 0 for key in dic: InfoEntropy += -1.0*dic[key]/num*np.log2(1.0*dic[key]/num) #********* End *********# return InfoEntropy#计算条件熵def calcHDA(feature, label, index, value): ''' input: feature(ndarray):样本特征 label(ndarray):样本标签 index(int):需要使用的特征列索引 value(int):index所表示的特征列中需要考察的特征值 output: HDA(float):信息熵 ''' #********* Begin *********# # sub_feature和sub_label表示根据特征列和特征值分割出的子数据集中的特征和标签 sub_feature = [] sub_label = [] for i in range(len(feature)): if feature[i,index] == value: sub_feature.append(feature[i]) sub_label.append(label[i]) sub_feature = np.array(sub_feature) sub_label = np.array(sub_label) HDA = calcInfoEntropy(sub_label) #********* End *********# return HDA#计算信息增益def calcInfoGain(feature, label, index): ''' input: feature(ndarry):测试用例中字典里的feature label(ndarray):测试用例中字典里的label index(int):测试用例中字典里的index，即feature部分特征列的索引。该索引指的是feature中第几个特征，如index:0表示使用第一个特征来计算信息增益。 output: InfoGain(float):信息增益 ''' #********* Begin *********# # 得到指定特征列的值的集合 dic = {} for i in feature[:,index]: if i not in dic: dic[i] = 1 else: dic[i] += 1 # 计算条件熵 HDA = 0 num = len(feature) for key in dic: HDA += 1.0*dic[key]/num*calcHDA(feature,label,index,key) # 计算信息增益 InfoGain = calcInfoEntropy(label) - HDA #********* End *********# return InfoGain#{'feature':[[0, 1], [1, 0], [1, 2], [0, 0], [1, 1]], 'label':[0, 1, 0, 0, 1], 'index': 0}#0.419973def getBestFeature(feature, label): ''' input: feature(ndarray):样本特征 label(ndarray):样本标签 output: best_feature(int):信息增益最高的特征 ''' #*********Begin*********# maxInfoGain,opIndex = 0,0 for index in range(feature.shape[1]): infoGain = calcInfoGain(feature,label,index) if infoGain &gt; maxInfoGain: maxInfoGain = infoGain opIndex = index #*********End*********# return opIndex#创建决策树def createTree(feature, label): ''' input: feature(ndarray):训练样本特征 label(ndarray):训练样本标签 output: tree(dict):决策树模型 ''' #*********Begin*********# # 样本里都是同一个label没必要继续分叉了 label_set = set(label) if(label_set.__len__()==1):return label[0] # 样本中只有一个特或者所有样本的特征都一样的话就看哪个label的票数高 flag = True arr = feature[0] for arrs in feature: if str(arr) != str(arrs) : flag = False if flag == True: dic = {} MAX = 0 l = 0 for i in label: if i not in dic: dic[i] = 1 else : dic[i] += 1 if dic[i] &gt; MAX: MAX = dic[i] l = i return l # 根据信息增益拿到特征的索引 best_feature = getBestFeature(feature,label) # 拿到bestfeature的所有特征值 v_set = set(feature[:,best_feature]) # 构建对应特征值的子样本集sub_feature, sub_label tree = {best_feature:{}} for value in v_set: sub_feature = [] sub_label = [] for i in range(len(feature)): if feature[i][best_feature] == value: sub_feature.append(feature[i]) sub_label.append(label[i]) sub_feature = np.array(sub_feature) sub_label = np.array(sub_label) # 递归构建决策树 tree[best_feature][value] = createTree(sub_feature,sub_label) #*********End*********# return tree#决策树分类def dt_clf(train_feature,train_label,test_feature): ''' input: train_feature(ndarray):训练样本特征 train_label(ndarray):训练样本标签 test_feature(ndarray):测试样本特征 output: predict(ndarray):测试样本预测标签 ''' #*********Begin*********# #创建决策树 nowTree = createTree(train_feature,train_label) #print(tree) #根据tree与特征进行分类 predict = [] for arr in test_feature: tree = nowTree.copy() print(tree) while(True): #print(tree) if type(tree).__name__ != 'dict': predict.append(tree) break keyList = list(tree.keys()) firstF = keyList[0] tree = tree[firstF][arr[firstF]] #*********End*********# return predictiris = load_iris()x = iris.datay = iris.targettrain_feature,test_feature,train_label,test_label = train_test_split(x,y,test_size=0.2,random_state=666)predict = dt_clf(test_feature,train_label,test_feature)print(predict)","link":"/2020/04/14/educoder%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5%EF%BC%9AID3%E5%86%B3%E7%AD%96%E6%A0%91/"},{"title":"educoder数据挖掘算法原理与实践：线性回归（房价预测）","text":"任务描述波士顿房价数据集共有506条波斯顿房价的数据，每条数据包括对指定房屋的13项数值型特征和目标房价组成。我们需要通过数据特征来对目标房价进行预测。数据集中部分数据与标签如下图所示: sklearn中已经提供了波士顿房价数据集的相关接口，想要使用该数据集可以使用如下代码： 123456from sklearn import datasets#加载波斯顿房价数据集boston = datasets.load_boston()#X表示特征，y表示目标房价x = boston.datay = boston.target 然后再对数据集进行划分： 123from sklearn.model_selection import train_test_split#划分训练集测试集，所有样本的20%作为测试集train_feature,test_feature,train_label,test_label = train_test_split(x,y,test_size=0.2,random_state=666) 线性回归算法原理模型训练流程由数据集可以知道，每一个样本有13个特征与目标房价，而我们要做的事就是通过这13个特征来预测房价，我们可以构建一个多元线性回归模型，来对房价进行预测。模型如下：$$y=b+w_1x_1+w_2x_2+w_3x_3+…+w_nx_n$$其中$x_i$表示第$i$个特征值，$w_i$表示第$i$个特征对应的权重，$b$表示偏置，$y$表示目标房价。为了方便，稍微变形$$y=w_0x_0+w_1x_1+w_2x_2+w_3x_3+…+w_nx_n$$其中$x_0=1$$$Y=\\theta X$$$$\\theta = (w_0,w_1,w_2,w_3,…,w_n)$$$$X=(1,x_1,x_2,x_3,…,x_n)$$而我们的目的就是找出能够正确预测的多元线性回归模型，即找出正确的参数 $\\theta$。那么如何寻找呢？通常在监督学习里面都会使用这么一个套路，构造一个损失函数，用来衡量真实值与预测值之间的差异，然后将问题转化为最优化损失函数。既然损失函数是用来衡量真实值与预测值之间的差异那么很多人自然而然的想到了用所有真实值与预测值的差的绝对值来表示损失函数。不过带绝对值的函数不容易求导，所以采用MSE(均方误差)作为损失函数，公式如下：$$loss = \\frac{1}{m}\\sum_{i=1}^m(y^i-p_i)^2$$其中$p$表示预测值，$y$表示真实值，$m$为样本总个数，$i$表示第$i$个样本。最后，我们再使用正规方程解来求得我们所需要的参数。线性回归模型训练流程图如下： 正规方程解对线性回归模型，假设训练集中$m$个训练样本，每个训练样本中有$n$个特征，可以使用矩阵的表示方法，预测函数可以写为：$$Y=θX$$ 其损失函数可以表示为 $$(Y−θX)^T(Y−θX)$$ 其中，标签 $Y$ 为 $m*1$的矩阵，训练特征 $X$ 为 $m*(n+1)$ 的矩阵，回归系数 $θ$ 为 $(n+1)x1$ 的矩阵，对 $θ$ 求导，并令其导数等于0，可以得到 $X^T(Y-\\theta X)=0$ 。所以，最优解为：$$\\theta=(X^TX)^{-1}X^TY$$这个就是正规方程解，我们可以通过最优方程解直接求得我们所需要的参数。 线性回归算法流程我们最终的目的是通过训练出来的线性回归模型对测试集数据进行预测，算法实现流程如下： 1234将x0=1加入训练数据使用正规方程解求得参数将x0=1加入测试数据对测试集数据进行预测 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import numpy as npfrom sklearn import datasets#msedef mse_score(y_predict,y_test): #********* Begin *********# mse = sum([(y-x)**2 for x,y in zip(y_predict,y_test)])/len(y_predict) #********* End *********# return msedef r2_score(y_predict,y_test): ''' input:y_predict(ndarray):预测值 y_test(ndarray):真实值 output:r2(float):r2值 ''' #********* Begin *********# a = sum([(y-x)**2 for x,y in zip(y_predict,y_test)]) b = sum([(np.mean(y_test)-y)**2 for y in y_test]) r2 = 1.0 - a/b #********* End *********# return r2def lr(train_feature,train_label,test_feature): ''' input: train_feature(ndarray):训练样本特征 train_label(ndarray):训练样本标签 test_feature(ndarray):测试样本特征 output: predict(ndarray):测试样本预测标签 ''' #********* Begin *********# #将x0=1加入训练数据 col1 = np.ones(len(train_feature)) train_feature = np.column_stack((train_feature,col1)); #使用正规方程解求得参数 theta = np.dot(np.dot(np.linalg.inv(np.dot(train_feature.T,train_feature)),train_feature.T),train_label) #将x0=1加入测试数据 col1 = np.ones(len(test_feature)) test_feature = np.column_stack((test_feature,col1)); #求得测试集预测标签 # print(theta) # print(test_feature) predict = np.dot(test_feature,theta) # print(predict) #********* End *********# return predict#加载波斯顿房价数据集boston = datasets.load_boston()#X表示特征，y表示目标房价x = boston.datay = boston.targetfrom sklearn.model_selection import train_test_split#划分训练集测试集，所有样本的20%作为测试集train_feature,test_feature,train_label,test_label = train_test_split(x,y,test_size=0.2,random_state=666)predict = lr(train_feature,train_label,test_feature)print(mse_score(predict,test_label))print(r2_score(predict,test_label))","link":"/2020/04/09/educoder%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%88%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%EF%BC%89/"},{"title":"在腾讯云服务器上部署hexo博客","text":"如何在腾讯云服务器上部署hexo博客本人在搭建过程中遇到了很多问题，因此写一个教程贴来总结一下 前期准备 本地环境 macOS Catalina 10.15 git node.js hexo 服务器端 腾讯云服务器 CentOS 7.6 64 Bit git node.js Nginx 服务器端部署 1.安装git1yum install git 2.安装node.js 1curl --silent --location https://rpm.nodesource.com/setup_5.x | bash - 使用 git --version 和 node --version 查看版本号 3.安装Nginx 1yuminstall nginx 使用 nginx -v查看版本号 4.配置Nginx 1vim /etc/nginx/nginx.conf 进行以下修改 1234567891011121314151617181920server { listen 80 default_server; listen [::]:80 default_server; server_name www.seven7.xyz seven7.xyz; #域名 root /home/hexo; #网站的根目录 自行创建 #以下不作修改 # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / { } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } 配置完成后保存退出，使用nginx -t查看配置是否有错误。查看运行状态：systemctl status nginx，显示running表示成功运行 5.创建git用户123useradd gitchmod chmod 740 /etc/sudoersvim /etc/sudoers 在sudoers中找到下面内容 12## Allow root to run any commands anywhereroot ALL=(ALL) ALL 并且在下面增加 1git ALL=(ALL) ALL 修改权限 1chmod 400 /etc/sudoers 使用su git切换到git用户 1234567891011# 切换到git用户目录cd /home/git# 创建.ssh文件夹mkdir ~/.ssh# 创建authorized_keys文件并编辑vim ~/.ssh/authorized_keys# 如果你还没有生成公钥，那么首先在本地电脑中执行 cat ~/.ssh/id_rsa.pub | pbcopy生成公钥# 再将公钥复制粘贴到authorized_keys# 保存关闭authorized_keys后，修改相应权限chmod 600 ~/.ssh/authorized_keyschmod 700 ~/.ssh 本地测试是否可以进行免密码登陆 1ssh -v git@服务器IP地址 6.建立git裸库1234# 回到git目录cd /home/git# 使用git用户创建git裸仓库，以blog.git为例git init --bare blog.git 使用下列命令修改用户组权限 12sudo chown git:git -R /home/hexosudo chown git:git -R /home/git/blog.git 7.使用git-hooks同步网站根目录1vim ~/blog.git/hooks/post-receive 在里面输入下列内容 12#!/bin/shgit --work-tree=/home/hexo --git-dir=/home/git/blog.git checkout -f 保存退出后执行 chmod +x post-receive修改权限 到这里服务器端就已经配置完成了 本地端配置 打开hexo根目录下的_config.yml文件，进行下列配置 1234deploy: type: git repo: git@你的服务器IP:/home/git/blog.git branch: master 然后发布 1hexo clean &amp;&amp; hexo g &amp;&amp; hexo d","link":"/2020/02/06/%E5%9C%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E9%83%A8%E7%BD%B2hexo%E5%8D%9A%E5%AE%A2/"},{"title":"educoder数据挖掘算法原理与实践：K均值","text":"什么是质心K-means算法是一个基于距离的聚类方法，距离指的是每个样本到质心的距离。那么，这里所说的质心是什么呢？ 其实，质心指的是样本每个特征的均值所构成的一个坐标。举个例子：假如有两个数据(1,1)和(2,2)则这两个样本的质心为(1.5,1.5)。 同样的，如果一份数据有m个样本，每个样本有n个特征，用$x_i^j$来表示第j个样本的第i个特征，则它们的质心为： $$Cmass=(\\frac{\\sum_{j=1}^mx_1^j}{m},\\frac{\\sum_{j=1}^mx_2^j}{m},…,\\frac{\\sum_{j=1}^mx_n^j}{m})$$ k-means算法原理K-means算法是基于数据划分的无监督聚类算法，首先定义常数k，常数k表示的是最终的聚类的类别数，在确定了类别数k后，随机初始化k个类别的聚类中心(质心)，通过计算每一个样本与聚类中心（质心）的距离，将样本点划分到距离最近的类别中。 k-means算法流程k-means算法流程如下： 1234随机初始k个点，作为类别中心。对每个样本将其标记为距离类别中心最近的类别。将每个类别的质心更新为新的类别中心。重复步骤2、3，直到类别中心的变化小于阈值。 过程如下图 如何确定k的值在K-means算法中，K值作为一个超参数，它的值需要我们自己来确定，通常k默认5。当然我们也可以写一个循环，将k等于各个值的结果输出，选择最好的结果的k值。部分代码如下： 12for i in range(k): km = KMmeans(i) 第一关代码1234567891011121314151617181920212223242526272829303132333435363738#计算样本间距离def distance(x, y, p=2): ''' input:x(ndarray):第一个样本的坐标 y(ndarray):第二个样本的坐标 p(int):等于1时为曼哈顿距离，等于2时为欧氏距离 output:distance(float):x到y的距离 ''' #********* Begin *********# if p == 1: return np.sum(np.abs(x-y)) else: return np.sum((x-y)**2)**0.5 #********* End *********##计算质心def cal_Cmass(data): ''' input:data(ndarray):数据样本 output:mass(ndarray):数据样本质心 ''' #********* Begin *********# Cmass = np.mean(data,axis=0) #********* End *********# return Cmass#计算每个样本到质心的距离，并按照从小到大的顺序排列def sorted_list(data,Cmass): ''' input:data(ndarray):数据样本 Cmass(ndarray):数据样本质心 output:dis_list(list):排好序的样本到质心距离 ''' #********* Begin *********# dis_list = [] for item in data: dis_list.append(distance(item,Cmass)) dis_list.sort() #********* End *********# return dis_list 第二关代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129# 计算一个样本与数据集中所有样本的欧氏距离的平方def euclidean_distance(one_sample, X): ''' input: one_sample(ndarray):单个样本 X(ndarray):所有样本 output: distances(ndarray):单个样本到所有样本的欧氏距离平方 ''' #*********Begin*********# distances = ((np.tile(one_sample,(len(X),1))-X)**2).sum(axis=1) #*********End*********# return distances# 从所有样本中随机选取k个样本作为初始的聚类中心def init_random_centroids(k,X): ''' input: k(int):聚类簇的个数 X(ndarray):所有样本 output: centroids(ndarray):k个簇的聚类中心 ''' #*********Begin*********# from random import sample centerIndexs = sample([x for x in range(len(X))],k) centroids = X[centerIndexs] #*********End*********# return centroids# 返回距离该样本最近的一个中心索引def _closest_centroid(sample, centroids): ''' input: sample(ndarray):单个样本 centroids(ndarray):k个簇的聚类中心 output: closest_i(int):最近中心的索引 ''' #*********Begin*********# closest_i = euclidean_distance(sample,centroids).argsort()[0] #*********End*********# return closest_i# 将所有样本进行归类，归类规则就是将该样本归类到与其最近的中心def create_clusters(k,centroids, X): ''' input: k(int):聚类簇的个数 centroids(ndarray):k个簇的聚类中心 X(ndarray):所有样本 output: clusters(list):列表中有k个元素，每个元素保存相同簇的样本的索引 ''' #*********Begin*********# clusters = [] for i in range(k): clusters.append([]) for i,sample in enumerate(X): clusters[_closest_centroid(sample,centroids)].append(i) #*********End*********# return clusters# 对中心进行更新def update_centroids(k,clusters, X): ''' input: k(int):聚类簇的个数 X(ndarray):所有样本 output: centroids(ndarray):k个簇的聚类中心 ''' centroids = [] #*********Begin*********# for i in range(k): nowSamples = X[clusters[i]] if(len(nowSamples)==0) : continue centroids.append(np.mean(nowSamples,axis=0)) #*********End*********# centroids = np.array(centroids) return centroids# 将所有样本进行归类，其所在的类别的索引就是其类别标签def get_cluster_labels(clusters, X): ''' input: clusters(list):列表中有k个元素，每个元素保存相同簇的样本的索引 X(ndarray):所有样本 output: y_pred(ndarray):所有样本的类别标签 ''' y_pred = np.zeros(len(X)) #*********Begin*********# for i,cluster in enumerate(clusters): for index in cluster: y_pred[index] = i #*********End*********# return y_pred# 对整个数据集X进行Kmeans聚类，返回其聚类的标签def predict(k,X,max_iterations,varepsilon): ''' input: k(int):聚类簇的个数 X(ndarray):所有样本 max_iterations(int):最大训练轮数 varepsilon(float):最小误差阈值 output: y_pred(ndarray):所有样本的类别标签 ''' #*********Begin*********# # 从所有样本中随机选取k样本作为初始的聚类中心 centroids = init_random_centroids(k, X) # 迭代，直到算法收敛(上一次的聚类中心和这一次的聚类中心几乎重合)或者达到最大迭代次数 for i in range(max_iterations): # 将所有进行归类，归类规则就是将该样本归类到与其最近的中心 clusters = create_clusters(k,centroids,X) # 计算新的聚类中心 newcentroids = update_centroids(k,clusters,X) # 如果聚类中心几乎没有变化，说明算法已经收敛，退出迭代 centroids = newcentroids k = len(centroids) y_pred = get_cluster_labels(clusters,X) #*********End*********# return y_pred","link":"/2020/04/15/educoder%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5%EF%BC%9AK%E5%9D%87%E5%80%BC/"},{"title":"关联规则挖掘-apriori算法","text":"算法原理什么是关联规则？关联规则分析也称为购物篮分析，最早是为了发现超市销售数据库中不同的商品之间的关联关系。关联规则是反映一个事物与其他事物之间的关联性，若多个事物之间存在着某种关联关系，那么其中的一个事物就能通过其他事物预测到。举个例子：在超市中，经常发现啤酒和尿布经常在同一张购物清单上出现，那是因为买尿裤的一般是有孩子的中年男性，他们在超市给孩子买尿裤时，会买上几瓶啤酒来犒劳自己，于是啤酒和尿布就相互关联。 几个重要概念 频繁项集是指经常出现在一块的物品的集合 两个不相交的非空集合 $X,Y$ ,如果有 $X-&gt;Y$ ，就说 $X-&gt;Y$ 是一条关联规则，比如买尿布的同时会买上啤酒( $尿布-&gt;啤酒$ )。关联规则的强度由支持度($support$)和可信度($confidence$)来描述 项集 $X,Y$ 同时发生的概率称为关联规则 $X-&gt;Y$ 的支持度($support$)$$ support(X-&gt;Y)=P(X \\cup Y)=\\frac{X,Y同时发生的事件个数}{总事件数} $$ 项集 $X,Y$ 发生的前提下，项集 Y 发生的概率称为关联规则 $X-&gt;Y$ 的可信度($confidence$)$$confidence(X-&gt;Y)=\\frac{P(X\\cup Y)}{P(X)}=\\frac{X,Y同时发生的事件个数}{X发生的事件个数}$$Apriori算法的原理如果某个项集是频繁项集，那么它所有的子集也是频繁的，就是说如果一个项集是非频繁的，那么它的所有超集也是非频繁的。基于此，Apriori算法从单元素项集开始，通过组合满足最小支持度的项集来形成更大的集合。因此Apriori算法的步骤可描述如下：1234(1)生成频繁1项集(2)通过K频繁项集生成K+1候选项集(3)由K+1候选项集通过计算支持度生成K+1频繁项集(4)重复(2)(3)直至候选项集为空 通过算法描述可以发现，Apriori算法在计算支持度时需要扫描数据库多遍。Apriori算法实现 calCntForEveryComb()函数用来求每条记录里所有组合的出现次数，通过二进制枚举计算，这个函数把所有组合的出现次数记录在字典里，以后需要的时候不需要在扫描数据库，支持获取即可，由于list不能被hash所以可以先将它强转成str类型。1234567891011121314def calCntForEveryComb(dataMat): cnt = {} for List in dataMat: n = len(List) for i in range(1,1&lt;&lt;n): comb = [] for j in range(0,n): if((1&lt;&lt;j)&amp;i): comb.append(List[j]) comb = sorted(comb) hashVal = hash(str(comb)) if hashVal not in cnt:cnt[hashVal] = 1 else:cnt[hashVal]+=1 return cnt calL1() 函数计算出来的是频繁1项集1234567def calL1(dataMat,cnt,support): L = [] for itemList in dataMat: for item in itemList: if [item] not in L and cnt[hash(str([item]))] &gt;= support: L.append([item]) return L createL()通过候选项集算出频繁项集12345678def createL(preL,cnt,support): L = [] curC = aprioriGen(preL) for itemList in curC: itemList = sorted(itemList) if hash(str(itemList)) in cnt and cnt[hash(str(itemList))] &gt;= support: L.append(itemList) return L aprioriGen() 函数通过频繁k-1项集自交计算出k候选项集，只有经过排序后k-1项相同的k-1频繁项集才可以相交12345678910def aprioriGen(preL): k = len(preL[0]) L = [] for i in range(0,len(preL)): for j in range(i+1,len(preL)): L1 = sorted(list(preL[i])[:k-1]) L2 = sorted(list(preL[j])[:k-1]) if L1 == L2: L.append(list(set(preL[i]).union(set(preL[j])))) return L Apriori() 是算法的主体函数，不断的进行 频繁项集-&gt;候选项集-&gt;频繁项集 的过程来生成频繁项集 12345678def Apriori(dataMat,cnt,support): Lk = calL1(dataMat,cnt,support) L = [] while True: Lk = createL(Lk,cnt,support) if(len(Lk)==0):break L = L + Lk return L 计算出的部分频繁项集： 生成关联规则挖掘关联规则原理如下：若某条规则不满足可信度要求，则该规则的所有子集也不满足可信度要求。例如：{A,B,C}-&gt;{D}是不满足可信度的，那么 {A,B}-&gt;{C,D}也是不满足的。 getRules() 用来获取关联规则，如果当前是频繁2项集，就直接计算关联规则，否则通过挖掘规则的原理进行剪枝挖掘。12345678def getRules(L,cnt,conf): rules = [] for itemList in L: itemSet = set(itemList) H1 = [set([item]) for item in itemSet] if(len(itemSet)==2):findRules(itemSet,H1,cnt,conf,rules) else:prunedRules(itemSet,H1,cnt,conf,rules) return rules findRules() 通过可信度计算出关联规则，同时将目前关联规则的右件保存用于剪枝12345678def findRules(itemSet,H,cnt,conf,rules): prunedList = [] for item in H: curConf = 1.0*cnt[hash(str(sorted(list(itemSet))))]/cnt[hash(str(sorted(list(itemSet-item))))] if(curConf&gt;=conf): rules.append((itemSet-item,item,curConf)) prunedList.append(item) return prunedList prunedRules() 函数通过不断的增大频繁项集的右件，使得右件由小到大的变化，从而获取该频繁项集下的所有符合要求的关联规则1234567def prunedRules(itemSet,H,cnt,conf,rules): k = len(H[0]) if len(itemSet) &gt; k: nextH = findRules(itemSet,H,cnt,conf,rules) if(len(nextH)==0): return nextH = [set(item) for item in aprioriGen(nextH)] if(len(nextH)&gt;0) : prunedRules(itemSet,nextH,cnt,conf,rules) 生成的部分关联规则： 完整代码click here","link":"/2020/03/03/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98-apriori%E7%AE%97%E6%B3%95/"},{"title":"关联规则挖掘-fpGrowth算法","text":"算法原理什么是关联规则？见 Apriori算法 几个重要概念见 Apriori算法 fp-growth算法介绍 打开你的搜索引擎，输入一个单词或一部分，例如“我”，搜索引擎可能会去统计和“我”一块出现得多的词，然后返回给你。其实就是去找频繁项集，而且需要相当地高效，像Apriori那样的速度肯定是不行的了。 本文要介绍的是FP-growth算法，它被用于挖掘频繁项集，它把数据集存储为一个叫FP树的数据结构里，这样可以更高效地发现频繁项集或频繁项对。相比于Apriori对每个潜在的频繁项集都扫描数据集判定是否满足支持度，FP-growth算法只需要遍历两次数据库，因此它在大数据集上的速度显著优于Apriori。 FP即Frequent Pattern，FP树看上去就是一棵前缀树，根节点是空集，结点上是单个元素，保存了它在数据集中的出现次数，出现次数越多的元素越接近根。此外，结点之间通过链接（link）相连，只有相似元素会被连起来，连起来的元素又可以看成链表。同一个元素可以在FP树中多次出现，根据位置不同，对应着不同的频繁项集。可以为FP树设置最小支持度，过滤掉出现次数太少的元素。 举个例子假设现有数据集 instance id elements 0 r, z, h, j, p 1 z, y, x, w, v, u, t, s 2 z 3 r, x, n, o, s 4 y, r, x, z, q, t, p 5 y, z, x, e, q, s, t, m 首先求出频繁1项集，并且将他们在每条记录里按出现次数降序排序，并且把不满足支持度的删除，假设支持度为3，则处理之后的数据集为 instance id elements 0 z,r 1 z, x, y, s, t 2 z 3 x, s, r 4 z, x, y, r, t 5 z, x, y, s, t fp树的结构和字典树trie一样，知道字典树的同学可以把它当作是字典树，和字典树不同的是，对于每一个元素，fp树有一个头指针，这个在后面会介绍到。 开始插入第一行记录，每经过一次节点，对应节点的值就加一同时头指针传递指向这个节点 插入第二行记录 全部插入之后 头指针用于生成条件模式基，通过头指针从一个叶节点到另一个叶节点开始向根部求解得到的前缀就是条件模式基 例如t的条件模式基： id elements count 0 z,x,y,s, 2 1 z,x,y,r 1 根据支持度删除掉不满足条件的项 id elements count 0 z,x,y 2 1 z,x,y 1 根据条件模式基创建条件模式树 通过不断的构造条件模式树得到频繁项集，这是通过t的条件模式树可以得到频繁项集{t},{t,y},{t,y,x},{t,y,x,z} fp-growth算法实现 树的存储形式采用对象的方式1234567class Node: def __init__(self,name,count,parent): self.name = name #节点名称 self.count = count #经过节点的次数 self.parent = parent #父节点 self.child = {} #子节点 self.nodeLink = None #头指针 createInitDateSet() 用于生成初始项集12345678def createInitDateSet(dataMat): dataDic = {} for itemList in dataMat: if frozenset(itemList) not in dataDic: dataDic[frozenset(itemList)] = 1 else: dataDic[frozenset(itemList)] += 1 return dataDic createTree() 用于创建fp树,首先获取头指针列表，删除不满足支持度的项,将所有记录里的项按出现次数降序排列，然后将所有记录插入到树中。12345678910111213141516171819202122232425262728293031def createTree(dataSet,sup): headTable = {} for itemList in dataSet: for item in itemList: headTable[item] = headTable.get(item,0) + dataSet.get(frozenset(itemList)) #print(len(headTable)) validKeys = [] for key in headTable.keys(): if(headTable[key]&lt;sup): validKeys.append(key) for key in validKeys: del headTable[key] #print(len(headTable)) freqItemSet = set(headTable.keys()) if(len(freqItemSet) == 0) : return None,None for key in headTable.keys(): headTable[key] = [headTable[key],None] root = Node('root',0,None) for itemDic,count in dataSet.items(): curItemDic = {} for item in itemDic: if item in freqItemSet: curItemDic[item] = headTable[item][0] if len(curItemDic) &gt; 0: sortedItems = [v[0] for v in sorted(curItemDic.items(), key=lambda x: x[1], reverse=True)] insertIntoTree(sortedItems,root,headTable,count) return root,headTable insertIntoTree() 用于将记录插入树中，插入过程和字典树的插入过程相同，首先判断该节点是否有这项，没有的话创建节点，有的话增加数目，之后递归插入。123456789101112def insertIntoTree(itemList,tree,headTable,count): item = itemList[0] if item in tree.child: tree.child[item].count += count else: tree.child[item] = Node(item,count,tree) if headTable[item][1] == None: headTable[item][1] = tree.child[item] else: updateHead(headTable[item][1],tree.child[item]) if len(itemList[1:])&gt;0: insertIntoTree(itemList[1:],tree.child[item],headTable,count) updateHead() 用于在插入的时候更新头指针，新的节点增加，需要使得头指针指向这个节点。1234def updateHead(headNode,treeNode): while(headNode.nodeLink != None): headNode = headNode.nodeLink headNode.nodeLink = treeNode createConTree() 用于创建条件模式树，首先获取每个头指针的条件模式集，同时保存频繁项集，创建条件模式树，如果条件模式树不为空，则递归求解12345678910111213def createConTree(FPtree,headTable,sup,preFix,freqItemList): #print(headTable) itemList = [v[0] for v in sorted(headTable.items(), key=lambda x: x[1][0])] for beginItem in itemList: newFreqSet = copy.deepcopy(preFix) newFreqSet.add(beginItem) freqItemList.append(newFreqSet) condBasePath = findPreFix(beginItem,headTable[beginItem][1]) #print(condBasePath) conTree,conHead = createTree(condBasePath,sup) if conTree != None: createConTree(conTree,conHead,sup,newFreqSet,freqItemList) findPreFix() 用于求解条件模式基，随着头指针的移动，递归求解出所有的条件模式基123456789def findPreFix(beginItem,LinkNode): condBasePath = {} while LinkNode != None: preFix = [] findParent(LinkNode,preFix) if len(preFix)&gt;1: condBasePath[frozenset(preFix[1:])] = LinkNode.count LinkNode = LinkNode.nodeLink return condBasePath findParent() 可以递归的向上求解前缀1234def findParent(curNode,preFix): if curNode.parent != None: preFix.append(curNode.name) findParent(curNode.parent,preFix) 求解出的部分频繁项集生成关联规则步骤与Apriori相同 见Apriori 完整代码click here","link":"/2020/03/03/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98-fpGrowth%E7%AE%97%E6%B3%95/"},{"title":"educoder数据挖掘算法原理与实践：k近邻","text":"KNN 算法思想k-近邻（k-nearest neighbor ,knn）是一种分类与回归的方法。我们这里只讨论用来分类的knn。所谓k最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最近的k个邻居来代表。 knn算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。knn方法在类别决策时，只与极少量的相邻样本有关。如上图，当k=3时离绿色的圆最近的三个样本中，有两个红色的三角形，一个蓝色的正方形，则此时绿色的圆应该分为红色的三角形这一类。而当k=5时，离绿色的圆最近的五个样本中，有两个红色的三角形，三个蓝色的正方形，则此时绿色的圆应该分为蓝色的正方形这一类 距离度量我们已经知道，如何判别一个样本属于哪个类型，主要是看离它最近的几个样本中哪个类型的数量最多，则该样本属于数量最多的类型。这里，有一个问题：何为最近？ 关于何为最近，大家应该自然而然就会想到可以用两个样本之间的距离大小来衡量，我们常用的有两种距离： 欧氏距离：欧氏距离是最容易直观理解的距离度量方法，我们小学、初中和高中接触到的两个点在空间中的距离一般都是指欧氏距离。二维平面上欧式距离计算公式：$$d_12=\\sqrt {(x_1^{(1)}-x_1^{(2)})^2+(x_2^{(1)}-x_2^{(2)})^2}$$n维平面上欧氏距离计算公式：$$d_12=\\sqrt {\\sum_{i=1}^{n}(x_i^{(1)}-x_i^{(2)})^2}$$ 曼哈顿距离：顾名思义，在曼哈顿街区要从一个十字路口开车到另一个十字路口，驾驶距离显然不是两点间的直线距离。这个实际驾驶距离就是“曼哈顿距离”。曼哈顿距离也称为“城市街区距离”。二维平面上曼哈顿距离计算公式：$$d_{12}=|x_1^{(1)}-x_1^{(2)}|+|x_2^{(1)}-x_2^{(2)}|$$n维平面上曼哈顿计算公式：$$d_{12}=\\sum_{i=1}^{n}|x_i^{(1)}-x_i^{(2)}|$$其中，上标圆括号内数字代表第几个样本，下标数字代表样本的第几个特征。 加权投票通过上面，我们已经知道如何找出最近的k个样本，但是，现在还有一个问题要我们来解决：如果有两个类型的样本数一样且最多，那么最终该样本应该属于哪个类型？ 其实，knn算法最后决定样本属于哪个类别，其实好比就是在投票，哪个类别票数多，则该样本属于哪个类别。而如果出现票数相同的情况，我们可以给每一票加上一个权重，用来表示每一票的重要性，这样就可以解决票数相同的问题了。很明显，距离越近的样本所投的一票应该越重要，此时我们可以将距离的倒数作为权重赋予每一票。如上图，虽然蓝色正方形与红色三角形数量一样，但是根据加权投票的规则，绿色的圆应该属于蓝色正方形这个类别。 KNN算法流程knn算法不需要训练模型，只是根据离样本最近的几个样本类型来判别该样本类型，所以流程非常简单： 123计算出新样本与每一个样本的距离找出距离最近的k个样本根据加权投票规则得到新样本的类别 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# encoding=utf8import numpy as npdef knn_clf(k,train_feature,train_label,test_feature): ''' input: k(int):最近邻样本个数 train_feature(ndarray):训练样本特征 train_label(ndarray):训练样本标签 test_feature(ndarray):测试样本特征 output: predict(ndarray):测试样本预测标签 ''' #*********Begin*********# #初始化预测结果 predict = [] #对测试集每一个样本进行遍历 for item in test_feature: #测试集第i个样本到训练集每一个样本的距离 dif = np.tile(item, (len(train_feature), 1)) - train_feature dif = dif ** 2 distance = (dif.sum(axis=1)) ** 0.5 #最近的k个样本的距离 #print(distance) topKDistance = np.sort(distance)[1:k+1] #最近的k个样本的索引 topKIndex = distance.argsort()[1:k+1] #最近的k个样本的标签 topKLabel = train_label[topKIndex] #初始化进行投票的字典，字典的键为标签，值为投票分数 dic = {} #初始化最大票数 Max = 0 bestLabel = 0 #进行投票 for i in range(0,k): #如果标签在字典的键中则投票计分 if topKLabel[i] in dic: dic[topKLabel[i]] += 1.0/topKDistance[i] #如果评分最高则将预测值更新为对应标签 if dic[topKLabel[i]] &gt; Max: Max = dic[topKLabel[i]] bestLabel = topKLabel[i] #如果标签不在字典中则将标签加入字典的键，同时计入相应的分数 else: dic[topKLabel[i]] = 1.0 / topKDistance[i] predict.append(bestLabel) #*********End*********# return predict#topK(i,k,x,y)from sklearn.datasets import load_digits#加载手写数字数据集digits = load_digits()#获取数据特征与标签x,y = digits .data,digits .targetfrom sklearn.model_selection import train_test_split#划分训练集测试集，其中测试集样本数为整个数据集的20%train_feature,test_feature,train_label,test_label = train_test_split(x,y,test_size=0.2,random_state=666)predict = knn_clf(5,train_feature,train_label,test_feature)sum = 0for i in range(len(test_feature)): if predict[i] == test_label[i]: sum += 1print(sum/len(test_label))","link":"/2020/04/15/educoder%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5%EF%BC%9Ak%E8%BF%91%E9%82%BB/"},{"title":"序列模型","text":"-– categories: “nlp” tags: - 序列模型 toc: true -– 序列模型序列模型举例 日常生活中的序列模型 大地震发生后，后面会跟着几次余震 预测股价 …… 自然语言中的序列模型 语言是连续的，“狗咬人”明显比“人咬狗”常见 音乐歌词是通顺的 …… 序列模型建模在时间$t$观察到$x_t$，那么得到$T$个不独立的随机变量$(x_1,…,x_T)～P(X)$,使用条件概率展开得到$p(a,b)=p(a)p(a|b)=p(b)p(a|b)$，那么有$$p(X) = p(x_1)·p(x_2|x_1)·p(x_3|x_1,x_2)·…·p(x_T|x_1,…,x_{T-1})$$也可以倒着推$$p(X) = p(x_T)·p(x_{T-1}|x_T)·p(x_{T-2}|x_{T-1},x_{T})·…·p(x_1|x_2,…,x_{T})$$但这个公式在实际中可以有意义也可能没意义，因此我们先只关注前向的公式 我们对条件概率建模$$P(x_t|x_1,…,x_{t-1}=p(x_t|f(x_1,…,x_{t-1})))$$那么$p(x_t)$仅仅取决于$f(x_1,…,x_{t-1})$，像这种对之前见过的数据建模，称为自回归模型 方案A - 马尔科夫假设本方案假设当前的状态只跟前面固定$\\tau$个状态有关系，即$$p(x_t|x_1,…,x_{t-1})=p(x_x|x_{t-\\tau},…,x_{t-1})=p(x_t|f(x_{t-\\tau},…,x_{t-1}))$$这种模型固定了影响因素的个数，因此可以用我们常用的一些算法来实现，比如最简单的MLP就可以，下面会有实现过程。 方案B - 潜变量模型引入潜变量$h_t$来表示过去的信息$h_t=f(x_1,…,x_{t-1})$，这样$x_t=p(x_t|h_t)$ 这种方案提出了两个问题： 由$x_{t-1},h_{t-1}$求$h_t$ 由$x_{t-1},h_{t}$求$x_t$ 马尔科夫假设的序列模型MLP的实现这里我们取$\\tau=4$，即当前状态仅和前面四个状态有关系。 预测的任务选择加入噪音的$sin$函数，使用前600个点进行训练。在预测的时候我们不断的使用已知的4个点去预测下一个点（这里我们所有的点都是已知的）。 预测的结果如图所示 其中蓝色的点为正常数据，橙色的点为我们预测的结果，从图上看大致可以预测正确。 这种预测是连续的情况下，即我们下面使用不连续的预测，即从第600个点开始使用$x_{t-4},x_{t-3},x_{t-2},x_{t-1}$预测$x_{t}$，然后使用预测出的$x_t$和$x_{t-3},x_{t-2},x_{t-1}$预测$t_{t+1}$,不断的这样迭代下去得出下图 我们发现从第600个点开始预测结果趋于-1，这是因为我们的误差在不断的累积。 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import torchfrom torch import nnimport matplotlib.pyplot as pltimport numpy as npdef init_weights(m): if type(m) == nn.Linear: nn.init.xavier_uniform_(m.weight)class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.Linear_1 = nn.Linear(4, 10) self.Linear_2 = nn.Linear(10, 1) self.activate = nn.ReLU() init_weights(self.Linear_1) init_weights(self.Linear_2) def forward(self,x): out = self.Linear_2(self.activate(self.Linear_1(x))) return outdef get_data(): T = 1000 time = torch.arange(1, T + 1, dtype=torch.float32) x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,)) tau = 4 features = torch.zeros((T - tau, tau)) for i in range(tau): features[:, i] = x[i:T - tau + i] labels = x[tau:].reshape((-1, 1)) train_data, train_labels = features[:600],labels[:600] return train_data, train_labels, features, time, xdef train(model, optimizer, loss, epochs, batch_size,train_data, train_labels): for epoch in range(epochs): for i in range(0,len(train_data),batch_size): optimizer.zero_grad() l = loss(model(train_data[i:i+batch_size]),train_labels[i:i+batch_size]) l.backward() optimizer.step() print(\"epoch{},loss:{}\".format(epoch,l))def predict(model, x): results = model(x) return resultsdef draw(x1, y1, x2, y2): plt.plot(x1, y1, label='real', linestyle='-') plt.plot(x2, y2, label='predict', linestyle=':') plt.show()def one_step_predict(model, feature): results = torch.zeros(400) # print(results) for i in range(600,1000): out = model(feature) results[i-600] = out feature = torch.cat((feature[1:], torch.tensor([out.item()])), -1) print(results) return resultsif __name__ == '__main__': train_data, train_labels, features, time, x = get_data() model = Model() optimizer = torch.optim.Adam(model.parameters(), 0.01) loss = nn.MSELoss() epochs = 5 batch_size = 16 train(model, optimizer, loss, epochs, batch_size,train_data, train_labels) results = predict(model, features) time_2 = torch.arange(5, 1001, dtype=torch.float32) draw(time, x, time_2, torch.tensor(results)) one_results = one_step_predict(model, features[600]) time_2 = torch.arange(600, 1000, dtype=torch.float32) draw(time, x, time_2, torch.tensor(one_results))","link":"/2021/09/12/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/"},{"title":"绕树三匝 何枝可依","text":"绕树三匝 何枝可依对于我来说，历时三个月的保研之路结束了，很幸运的最后获得了梦想学校的offer，三个月我心路历程，希望能通过文字的方式记录下来。 夏令营对于我的出身，夏令营不抱有太多的希望。但是出现了战略性的失误，对于我的水平，夏令营应该用来找保底的院校，而不是以上岸为目的。于是所投院校没有一个获得复试机会。但是还是通过本校老师的帮助获得了中科院计算所的复试机会，在此感谢nwj老师的帮助！ 中科院计算所计算所的复试在7月底，这个时间段的我时间都用在了准备期末考试和最后的小学期，以至于根本没有复习，我也没有想到nwj老师给我联系的课题组是计算所中最热门实验室的最热门课题组，所以后面的结果可想而知。 该课题组的考核方式是笔试+机试+面试。笔试和机试的分数用来进行第一批筛人。笔试做的一塌糊涂，索性机试做的还算可以，题目都做了出来。笔试和机试刷了30%的人，虽然我进入了面试，但是排名不高。这时我已经想放弃了，但是还是决定去试一试。在面试过程中我出现了能力范围之外的失误，进入面试会议室之后，不知道什么原因我的耳机坏掉了，于是我感觉关闭了耳机，没想到使用电脑音频后，声音小的可怜，我需要趴在喇叭上才能听到老师的声音。之后要共享屏幕进行ppt的展示，但是我居然忘了给腾讯会议分享屏幕的权限，只能退出腾讯会议，给了权限之后重新进入会议室。这时我的心态就可以有点崩了，在印象分上我就已经失败了。讲完ppt之后，我没想到紧接而来的是老师的压力面。上来就“嘲讽”我笔试之烂，英语水平之烂，然后问了我两个算法题，索性答了上来，挽回一些局面，之后就是一些闲聊。当天晚上出了结果，很正常的被刷掉了。之后我计算所联系的老师也来责问我的耳机为什么坏了，为什么这么不重视这场面试？ 之后我也给计算所的其他实验室发了邮件争取面试机会，可是无一回信。计算所的夏令营之旅就这样结束了。最后让我感动苦恼的不是我的个人能力配不上那个课题组，而是我在大事情面前的不细心，一些很小的错误我都解决不了。几天后我坐在楼梯上，眼泪不知道怎么的就流了下来，心来想着我就那么没用吗？我还能找到学校吗？ 预推免中山大学智能工程学院在群里看到有人发一个老师(s)的的招生信息，方向是nlp，于是赶紧发了邮件介绍自己。简单了聊了一下，老师给了我两道题目，过了一个星期我做完给s发了过去，s说做的不错，承诺给我一个名额。和s的交流过程中可以看出她的能力很强，人也很好。 但是我学院的初审没过，只能和s老师无缘了，s老师也发了微信安慰我。 重庆大学重庆大学联系了一个人很好的老师g老师，进行了一个简短的面试并给了一个考核任务，最后老师说想要签写一个录取的协议，但感觉为了一个保底而选择签协议的代价太大了，于是放弃。 中国科学院信息工程研究所我投了信工所二室和三室，二室被刷了，三室也被刷了，但是最后y老师给我发了一封邮件说他那里有一个直博的名额，问我愿不愿意试一试，当时我的还处于0offer状态，于是和老师打电话聊了半个小时，进入了三室的面试。面试过后我问y老师能否通过，他说面试的时候他没在那想要和我在面一次，第二天早上他和一位学长对我进行了长达1个小时40分钟的面试，面试过后两天，信工所发来了录取邮件，终于有了自己的第一个offer，有开心也有不开心，不开心的是自己不甘心于此。最后也是放弃了这个offer，这里对y老师说声抱歉！ 中南大学发邮件联系了一个中南大学的老师，进行了电话面试，但是上来他就说我打的这些比赛没用，他需要的是做科研的不是打比赛的，于是我辩解了一下，然后又问了我很多方面，最后他说我对你很满意，但是你需要通过中南大学的预推免才可以来。最后也是没报名中南大学的预推免。 山东大学山东大学联系了今年的青橙奖得主n老师，安排了一场面试，面试感觉表现的不错，实验室给了一个专硕的名额，但是没想到我tm初审没过。。。所以gg。 浙江大学软件学院之前考了浙江大学的计算机等级能力测试PAT考试，获得了满分的成绩，这个可以抵机试分数，感觉通过复试的问题不大。我是当天第一个面试的，最后结果出来后，面试分很低但是还是获得了录取的资格。最后放弃。 电子科技大学电子科技大学我联系了一个去年的青橙奖得主g老师，进行一场面试后老师同意接收我进实验室，让我去报名电科的预推免，由于当时获得了浙大的offer，于是放弃了电科。 同济大学通过本校老师n老师的帮助联系了同济大学的f老师，老师也同意了我报考她的研究生，最后也获得了复试机会，但最后放弃。 华东师范大学获得复试机会，但最终放弃。 中科院计算所经过夏令营的失利，感觉我的能力可能无法进入计算所，于是就想着碰碰运气，能上就上，上不了我也不后悔。选择了另一个热门的实验室报名，获得了面试机会，在面试过程中可以感觉到老师在敷衍我，感觉没戏了，最后也是没获得录取资格。 过了一个星期，在当我迷茫之时，收到了l老师的短信： 我异常激动，马上和老师进行了联系，聊了半个小时后老师说需要开会讨论一下，晚上收到了老师发来的短信说已经同意录取我。我简直开心到爆了！最终选择了去计算所就读。 至此，我的推免之路应该就告一段落了！ 总结无数的失眠夜，三年以来的努力，最近几个月的煎熬最终在点击接收待录取的按钮之时得到了解释。绕树三匝，终寻一枝。感谢各位老师的帮助，感谢家人们，朋友们的陪伴！ 希望多年之后我在翻开这篇总结，可以热泪盈眶。 路漫漫其修远兮，吾将上下而求索！","link":"/2020/10/15/%E7%BB%95%E6%A0%91%E4%B8%89%E5%8C%9D%20%E4%BD%95%E6%9E%9D%E5%8F%AF%E4%BE%9D/"},{"title":"神经网络作业二","text":"题意考虑以下的二类训练样本集 Instance Feature vector Output label 1 (0,0) + 2 (1,0) + 3 (0,1) - 4 (-1,0) - 5 (1,-1) - 对此训练样本集，我们需要训练一个三层神经网络（输入层、单隐层、输出层），其中单隐层的单元（神经元）数目设为2，激活函数（activation function）为Sigmoid函数： （1）在二维坐标系中画出这5个训练样本点，并讨论此训练样本集是否线性可分。（2）试分析将Sigmoid激活函数换成线性函数的缺陷。（3）令初始化参数全部为0，试运用前馈（feedforward）算法计算在初始化参数下此三层神经网络的输出；然后运用反向传播（backpropagation）算法，计算代价函数对所有参数的偏导数，并讨论将初始化参数全部设为0所带来的问题。（4）试给出一个神经网络（画出架构图，并写出激活函数及其对应的参数），使此训练样本集的5个训练样本点都可以被正确分类。 答案**(1)** 作出的图如下，显然不存在一条直线可以将样本实现正确的分类。**(2)** 如果激活函数换成线性函数，那么无论神经网络有多少层，输出都是输入的线性组合，与没有隐藏层的效果相当，就成了最原始的感知器了，与不使用激活函数、直接使用逻辑回归没有区别。**(3)** 由题意知$w_{i,j}=0,\\theta_i=0$使用第一条数据进行计算,输入$x_1=0,x_2=0,y=0$ 正向传播$I_3= x_1*w_{13}+x_2*w_{23}+\\theta_3=0*0+0*0+0=0$ $O_3 = sigmoid(I_3) = 0.5$ 同理$I_4 =0$$O_4 = 0.5$$I_5 =0$$O_5 = 0.5$ 反向传播计算误差对各参数的偏导数$E=\\frac{1}{2}*(target_5-O_5)^2$ $\\displaystyle\\frac{\\partial E}{\\partial w_{35}}=\\displaystyle\\frac{\\partial I_5}{\\partial w_{35}}*\\displaystyle\\frac{\\partial O_5}{\\partial I_5}*\\displaystyle\\frac{\\partial E}{\\partial O_5}$ $\\displaystyle\\frac{\\partial E}{\\partial O_5}=-(target_5-O_5)$ $\\displaystyle\\frac{\\partial O_5}{\\partial I_5}=\\displaystyle\\frac{\\partial \\frac{1}{1+e^{-I_5}}}{\\partial I_5}=O_5*(1-O_5)$ $\\displaystyle\\frac{\\partial I_5}{\\partial w_{35}}=\\displaystyle\\frac{\\partial (w_{35}*O_3+w_{45}*O_4+\\theta_5*1)}{\\partial w_{35}}=O_3$ $\\therefore \\displaystyle\\frac{\\partial E}{\\partial w_{35}}=-O_5*(1-O_5)*(target_5-O_5)*O_3=-0.0625$ $\\displaystyle\\frac{\\partial E}{\\partial \\theta_5}=\\displaystyle\\frac{\\partial I_5}{\\partial \\theta_5}*\\displaystyle\\frac{\\partial O_5}{\\partial I_5}*\\displaystyle\\frac{\\partial E}{\\partial O_5}$ $=\\displaystyle\\frac{\\partial I_5}{\\partial \\theta_5}*\\displaystyle\\frac{\\partial O_5}{\\partial I_5}*\\displaystyle\\frac{\\partial (w_{35}*O_3+w_{45}*O_4+\\theta_5*1)}{\\partial \\theta_5}$ $=\\displaystyle\\frac{\\partial I_5}{\\partial \\theta_5}*\\displaystyle\\frac{\\partial O_5}{\\partial I_5}$ $=-O_5*(1-O_5)*(target_5-O_5)=-0.125$ 同理 $\\displaystyle\\frac{\\partial E}{\\partial w_{45}}=-0.0625$ $\\displaystyle\\frac{\\partial E}{\\partial w_{13}}=0$ $\\displaystyle\\frac{\\partial E}{\\partial w_{14}}=0$ $\\displaystyle\\frac{\\partial E}{\\partial w_{23}}=0$ $\\displaystyle\\frac{\\partial E}{\\partial w_{24}}=0$ $\\displaystyle\\frac{\\partial E}{\\partial \\theta_4}=0$ $\\displaystyle\\frac{\\partial E}{\\partial \\theta_3}=0$ 初始化参数全部设为0所带来的问题如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值，那么隐藏神经元对输出单元的影响也是相同的，通过反向传播梯度下降法进行计算时，会得到同样的梯度大小，所以无论设置多少个隐藏单元，其最终的影响都是相同的。同理，也不能初始化所有的参数都为同一个非0的数。因此，要随机化初始参数，以打破对称性。 (4) 经训练后得到参数如下: $w_{ 1 3 }= -7.204$ $w_{ 2 3 }= 7.898$ $\\theta_ 3 = 3.295$ $w_{ 1 4 }= 0.139$ $w_{ 2 4 }= 6.273$ $\\theta_4 = -2.906$ $w_{ 3 5 }= -9.242$ $w_{ 4 5 }= 8.832$ $\\theta_5 = 3.967$ 预测结果:通过结果来看，5个训练样本点都可以被正确分类 **完整代码**click here","link":"/2020/03/06/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%9A%E4%BA%8C/"},{"title":"神经网络作业一","text":"题目 将数据映射到[0-1] department 映射前 映射后 sales 0 systems 0.33 marketing 0.67 secretary 1 status 映射前 映射后 senior 0 junior 1 age 映射前 映射后 21···25 0 26···30 0.25 31···35 0.5 36···40 0.75 41···45 1 salary 映射前 映射后 26K···30K 0 31K···35K 0.125 36K···40K 0.250 41K···45K 0.375 46K···50K 0.500 51K···55K 0.625 56K···60K 0.750 61K···65K 0.875 66K···70K 1 构建前馈神经网络 $w_{14} = 0.07000$ $w_{15} = 0.23000$ $w_{16} = 0.30000$ $w_{17} = 0.44000$$w_{24} = 0.23000$ $w_{25} = 0.40000$ $w_{26} = 0.42000$ $w_{27} = 0.37000$$w_{34} = 0.27000$ $w_{35} = 0.40000$ $w_{36} = 0.03000$ $w_{37} = 0.09000$$w_{48} = -0.10000$ $w_{58} = -0.49000$ $w_{68} = 0.16000$ $w_{78} = 0.47000$ $\\theta_4 = 0.12$ $\\theta_5 = -0.1$ $\\theta_6 = 0.29$ $\\theta_7 = -0.29$ $\\theta_8 = 0.17$ 测试数据为 (sales,senior,31···35,46K···50K)映射之后为(0,0,0.5,0.5)其中输入为(0,0.5,0.5)输出为0 计算各个节点的输出$I_4 = w_{14}*x_1+w_{24}*x_2+w_{34}*x_3 + heta_4 = 0.370$$O_4 = \\displaystyle\\frac{1}{1+e^-{I_4}}=0.591$ $I_5 = w_{15}*x_1+w_{25}*x_2+w_{35}*x_3 + heta_5 = 0.300$$O_5 = \\displaystyle\\frac{1}{1+e^-{I_5}}=0.574$ $I_6 = w_{16}*x_1+w_{26}*x_2+w_{36}*x_3 + heta_6 = 0.515$$O_6 = \\displaystyle\\frac{1}{1+e^-{I_6}}=0.626$ $I_7 = w_{17}*x_1+w_{27}*x_2+w_{37}*x_3 + heta_7 = -0.060$$O_7 = \\displaystyle\\frac{1}{1+e^-{I_7}}=0.485$ $I_7 = w_{48}*O_4+w_58*O_5+w_{68}*O_6 + w_{78}*O_7 + \\theta_8 = 0.398$$O_8 = \\displaystyle\\frac{1}{1+e^-I_8}=0.598$ 计算误差$Err_8 = O_8(1-O_8)(T_8-O_8)=-0.144$ $Err_7 = O_7(1-O_7)(\\sum_kErr_kw_{jk})=O_7(1-O_7)(Err_8)(W_{78})=-0.017$ $Err_6 = O_6(1-O_6)(\\sum_kErr_kw_{jk})=O_6(1-O_6)(Err_8)(W_{68})=-0.005$ $Err_5 = O_5(1-O_5)(\\sum_kErr_kw_{jk})=O_5(1-O_5)(Err_8)(W_{58})=0.017$ $Err_4 = O_4(1-O_4)(\\sum_kErr_kw_{jk})=O_4(1-O_4)(Err_8)(W_{48})=0.003$ 更新权值和偏置值 权值 $w_{14} = w_{14}+(l)*Err_4*O_1=0.070$ $w_{15} = w_{15}+(l)*Err_5*O_1=0.230$ $w_{16} = w_{16}+(l)*Err_6*O_1=0.300$ $w_{17} = w_{17}+(l)*Err_7*O_1=0.440$ $w_{24} = w_{24}+(l)*Err_4*O_2=0.232$ $w_{25} = w_{25}+(l)*Err_5*O_2=0.408$ $w_{26} = w_{26}+(l)*Err_6*O_2=0.418$ $w_{27} = w_{27}+(l)*Err_7*O_2=0.362$ $w_{34} = w_{34}+(l)*Err_4*O_3=0.272$ $w_{35} = w_{35}+(l)*Err_5*O_3=0.408$ $w_{36} = w_{36}+(l)*Err_6*O_3=0.028$ $w_{37} = w_{37}+(l)*Err_7*O_3=0.082$ $w_{48} = w_{48}+(l)*Err_8*O_4=-0.177$ $w_{58} = w_{58}+(l)*Err_8*O_5=-0.564$ $w_{68} = w_{68}+(l)*Err_8*O_6=0.079$ $w_{78} = w_{78}+(l)*Err_8*O_7=0.407$ $w_{48} = w_{48}+(l)*Err_8*O_4=-0.177$ $w_{58} = w_{58}+(l)*Err_8*O_5=-0.564$ $w_{68} = w_{68}+(l)*Err_8*O_6=0.079$ $w_{78} = w_{78}+(l)*Err_8*O_7=0.407$ $w_{48} = w_{48}+(l)*Err_8*O_4=-0.177$ $w_{58} = w_{58}+(l)*Err_8*O_5=-0.564$ $w_{68} = w_{68}+(l)*Err_8*O_6=0.079$ $w_{78} = w_{78}+(l)*Err_8*O_7=0.407$ $w_{48} = w_{48}+(l)*Err_8*O_4=-0.177$ $w_{58} = w_{58}+(l)*Err_8*O_5=-0.564$ $w_{68} = w_{68}+(l)*Err_8*O_6=0.079$ $w_{78} = w_{78}+(l)*Err_8*O_7=0.407$ $w_{48} = w_{48}+(l)*Err_8*O_4=-0.177$ $w_{58} = w_{58}+(l)*Err_8*O_5=-0.564$ $w_{68} = w_{68}+(l)*Err_8*O_6=0.079$ $w_{78} = w_{78}+(l)*Err_8*O_7=0.407$ 偏置值 $\\theta_4 = \\theta_4+(l)*Err_4=0.123$ $\\theta_5 = \\theta_5+(l)*Err_5=-0.084$ $\\theta_6 = \\theta_6+(l)*Err_6=0.285$ $\\theta_7 = \\theta_7+(l)*Err_7=-0.305$ $\\theta_8 = \\theta_8+(l)*Err_8=0.041$ 完整代码click here","link":"/2020/03/05/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BD%9C%E4%B8%9A%E4%B8%80/"},{"title":"爬虫入门之从当当网爬取图片","text":"1234567891011121314151617181920212223242526272829303132import reimport urllib.request#获取html页面def get_html(url): page = urllib.request.urlopen(url) html_a = page.read() #根据网页编码设置编码方式 return html_a.decode('gbk')def get_img(html): #封装正则表达式对象 reg = r'http://[^\\s]*?\\.jpg' imgre = re.compile(reg) #获取所有的图片url imglist = imgre.findall(html) x = 0 path = '/Users/seven7777777/code/' for imgurl in imglist: #将url以图片形式存储到本地 urllib.request.urlretrieve(imgurl, '{0}{1}.jpg'.format(path, x)) x = x + 1 return imglistif __name__ == '__main__': url = 'http://www.dangdang.com' #get_html(url) print (get_img(get_html(url)))","link":"/2020/02/20/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E4%B9%8B%E4%BB%8E%E5%BD%93%E5%BD%93%E7%BD%91%E7%88%AC%E5%8F%96%E5%9B%BE%E7%89%87/"},{"title":"贝叶斯分类器","text":"地震预报是比较困难的一个课题,可以根据地震与生物异常反应之间的联系来进行研究。根据历史记录的统计,地震前一周内出现生物异常反应的概率为50%,而一周内没有发生地震但也出现了生物异常反应的概率为 10%。假设某 一个地区属于地震高发区,发生地震的概率为 20%。问:如果某日观察到明显的生物异常反应现象,是否应当预报一周内将发生地震? 设发生地震为$w_1$，不发生地震为$w_2$，当前出现的状态(出现了生物异常)为$x$。由题意可得$P(x|w_1)=0.5,P(x|w_2)=0.1,P(w_1)=0.2,P(w_2)=0.8$ 当前情况下$w_1$的概率为$P(w_1|x)=\\displaystyle\\frac{P(x|w_1)P(w_1)}{P(x)}$ 当前情况下$w_2$的概率为$P(w_2|x)=\\displaystyle\\frac{P(x|w_2)P(w_2)}{P(x)}$ $P(x)=\\sum_{j=1}^2P(x|w_j)P(w_j)=0.18$$P(w_1|x)=\\displaystyle\\frac{P(x|w_1)P(w_1)}{P(x)}=0.5556$$P(w_2|x)=\\displaystyle\\frac{P(x|w_2)P(w_2)}{P(x)}=0.4444$所以$P(w_1|x)=0.5556&gt;P(w_2|x)=0.4444$，故应当预报一周内将发生地震 对于上例中的地震预报问题,假设预报一周内发生地震,可以预先组织抗震 救灾,由此带来的防灾成本会有 2500 万元,而当地震确实发生时,由于地震造 成的直接损失会有 1000 万元;假设不预报将发生地震而地震又发生了,造成的 损失会达到 5000 万元。请问在观察到明显的生物异常反应后,是否应当预报一周内将发生地震? 根据题意可得：实际发生地震且预报发生地震的损失$\\lambda_1^1=3500$实际发生地震却不预报发生地震的损失$\\lambda_1^2=5000$实际不发生地震却预报发生地震的损失$\\lambda_2^1=2500$实际不发生地震且不预报发生地震的损失$\\lambda_2^2=0$ $R(\\alpha_1|X)$$=\\sum_{j=1}^2\\lambda_j^1P(w_j|x)$$=\\lambda_1^1P(w_1|x)+\\lambda_2^1P(w_2|x)$$=3500*\\displaystyle\\frac{P(x|w_1)P(w_1)}{P(x)}+2500*\\displaystyle\\frac{P(x|w_2)P(w_2)}{P(x)}$$=\\displaystyle\\frac{550}{P(x)}=3055.6$ $R(\\alpha_2|X)$$=\\sum_{j=1}^2\\lambda_j^2P(w_j|x)$$=\\lambda_1^2P(w_1|x)+\\lambda_2^2P(w_2|x)$$=5000*\\displaystyle\\frac{P(x|w_1)P(w_1)}{P(x)}$$=\\displaystyle\\frac{500}{P(x)}=2777.8$ 故$R(\\alpha_1|X)=3055.6&gt;R(\\alpha_2|X)=2777.8$，所以不应当预报一周内将发生地震","link":"/2020/02/19/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/"},{"title":"资源分享","text":"免费图床catbox、免注册、最大支持200M click here","link":"/2020/02/28/%E8%B5%84%E6%BA%90%E5%88%86%E4%BA%AB/"}],"tags":[{"name":"MAC","slug":"MAC","link":"/tags/MAC/"},{"name":"决策树","slug":"决策树","link":"/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"name":"CART","slug":"CART","link":"/tags/CART/"},{"name":"贝叶斯定理","slug":"贝叶斯定理","link":"/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86/"},{"name":"ID3","slug":"ID3","link":"/tags/ID3/"},{"name":"educoder","slug":"educoder","link":"/tags/educoder/"},{"name":"多元线性回归","slug":"多元线性回归","link":"/tags/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"K-means","slug":"K-means","link":"/tags/K-means/"},{"name":"关联规则挖掘","slug":"关联规则挖掘","link":"/tags/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98/"},{"name":"KNN","slug":"KNN","link":"/tags/KNN/"},{"name":"保研","slug":"保研","link":"/tags/%E4%BF%9D%E7%A0%94/"},{"name":"神经网络","slug":"神经网络","link":"/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"爬虫","slug":"爬虫","link":"/tags/%E7%88%AC%E8%99%AB/"},{"name":"不定时更新","slug":"不定时更新","link":"/tags/%E4%B8%8D%E5%AE%9A%E6%97%B6%E6%9B%B4%E6%96%B0/"}],"categories":[{"name":"教程","slug":"教程","link":"/categories/%E6%95%99%E7%A8%8B/"},{"name":"数据挖掘","slug":"数据挖掘","link":"/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"推荐系统","slug":"推荐系统","link":"/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"总结","slug":"总结","link":"/categories/%E6%80%BB%E7%BB%93/"},{"name":"学习笔记","slug":"学习笔记","link":"/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"资源总结","slug":"资源总结","link":"/categories/%E8%B5%84%E6%BA%90%E6%80%BB%E7%BB%93/"}]}